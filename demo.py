import os
import argparse
import numpy as np
import tensorflow.compat.v1 as tf
import matplotlib.pyplot as plt
import matplotlib
import cv2
from scipy import ndimage

# Configure Chinese font support for matplotlib
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# Disable TF 2.x behavior for compatibility
tf.disable_v2_behavior()

from PIL import Image
import numpy as np

# OCR utilities
from utils.ocr_enhanced import extract_room_text, fuse_ocr_and_segmentation, set_closet_enabled
from utils.rgb_ind_convertor import floorplan_fuse_map, floorplan_fuse_map_figure

def imread(path, mode='RGB'):
    """Read image using PIL"""
    img = Image.open(path)
    if mode == 'RGB':
        img = img.convert('RGB')
    elif mode == 'L':
        img = img.convert('L')
    return np.array(img)

def imsave(path, img):
    """Save image using PIL"""
    if img.dtype != np.uint8:
        img = (img * 255).astype(np.uint8)
    Image.fromarray(img).save(path)

def imresize(img, size):
    """Resize image using PIL"""
    # Convert to uint8 if needed
    if img.dtype != np.uint8:
        if img.max() <= 1.0:
            img = (img * 255).astype(np.uint8)
        else:
            img = img.astype(np.uint8)
    
    if len(img.shape) == 3:
        h, w, c = size if len(size) == 3 else (*size, img.shape[2])
        img_pil = Image.fromarray(img)
        img_resized = img_pil.resize((w, h))
        return np.array(img_resized)
    else:
        h, w = size
        img_pil = Image.fromarray(img)
        img_resized = img_pil.resize((w, h))
        return np.array(img_resized)
from matplotlib import pyplot as plt
import matplotlib
# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·

# Force CPU usage - disable GPU 
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Disable GPU

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# input image path
parser = argparse.ArgumentParser()

parser.add_argument('--im_path', type=str, default='./demo/45765448.jpg',
                    help='input image paths.')
parser.add_argument('--disable_closet', action='store_true',
                    help='map closet predictions to background')


def simple_connected_components(mask):
    """Simple connected component labeling without scipy"""
    h, w = mask.shape
    labels = np.zeros_like(mask, dtype=int)
    current_label = 0
    
    def flood_fill(start_y, start_x, label):
        """Flood fill algorithm for connected components"""
        stack = [(start_y, start_x)]
        while stack:
            y, x = stack.pop()
            if (y < 0 or y >= h or x < 0 or x >= w or 
                labels[y, x] != 0 or not mask[y, x]):
                continue
            
            labels[y, x] = label
            # Add 4-connected neighbors
            stack.extend([(y+1, x), (y-1, x), (y, x+1), (y, x-1)])
    
    for y in range(h):
        for x in range(w):
            if mask[y, x] and labels[y, x] == 0:
                current_label += 1
                flood_fill(y, x, current_label)
    
    return labels, current_label

def apply_precise_kitchen_coordinates(floorplan, ocr_results, ori_shape):
    """ç²¾ç¡®åˆ†æOCRè¯†åˆ«çš„å¨æˆ¿æ–‡å­—ä½ç½®ï¼Œè¯¦ç»†è¾“å‡ºåæ ‡è½¬æ¢è¿‡ç¨‹"""
    if not ocr_results:
        return floorplan, []
    
    # æŸ¥æ‰¾å¨æˆ¿OCRç»“æœ
    kitchen_boxes = []
    for ocr_item in ocr_results:
        text = ocr_item['text'].lower()
        if any(keyword in text for keyword in ['å¨æˆ¿', 'kitchen', 'cook', 'çƒ¹é¥ª']):
            x, y, w, h = ocr_item['bbox']
            
            print(f"ğŸ” OCRå¨æˆ¿è¯†åˆ«è¯¦ç»†åˆ†æ:")
            print(f"   æ£€æµ‹åˆ°æ–‡å­—: '{ocr_item['text']}'")
            print(f"   ç½®ä¿¡åº¦: {ocr_item['confidence']:.3f}")
            print(f"   ğŸ¯ OCRåŸå§‹æ•°æ®åˆ†æ:")
            print(f"      è¾¹ç•Œæ¡†(x,y,w,h): ({x}, {y}, {w}, {h})")
            print(f"      æ–‡å­—åŒºåŸŸ: å·¦ä¸Šè§’({x}, {y}) -> å³ä¸‹è§’({x+w}, {y+h})")
            
            # è®¡ç®—OCRæ£€æµ‹åˆ°çš„"å¨æˆ¿"æ–‡å­—çš„ç²¾ç¡®ä¸­å¿ƒ
            ocr_center_x = x + w // 2
            ocr_center_y = y + h // 2
            
            print(f"   ğŸ“ OCRæ–‡å­—ä¸­å¿ƒè®¡ç®—:")
            print(f"      ä¸­å¿ƒX = {x} + {w}//2 = {ocr_center_x}")
            print(f"      ä¸­å¿ƒY = {y} + {h}//2 = {ocr_center_y}")
            print(f"      OCRæ–‡å­—ä¸­å¿ƒ: ({ocr_center_x}, {ocr_center_y}) [512x512åæ ‡ç³»]")
            
            # è½¬æ¢ä¸ºåŸå§‹å›¾åƒåæ ‡
            orig_center_x = int(ocr_center_x * ori_shape[1] / 512)
            orig_center_y = int(ocr_center_y * ori_shape[0] / 512)
            
            print(f"   ğŸ”„ åæ ‡ç³»è½¬æ¢:")
            print(f"      åŸå§‹å›¾åƒå°ºå¯¸: {ori_shape[1]} x {ori_shape[0]}")
            print(f"      512x512 -> åŸå§‹å›¾åƒè½¬æ¢æ¯”ä¾‹:")
            print(f"        Xæ¯”ä¾‹: 512 -> {ori_shape[1]} (Ã—{ori_shape[1]/512:.3f})")
            print(f"        Yæ¯”ä¾‹: 512 -> {ori_shape[0]} (Ã—{ori_shape[0]/512:.3f})")
            print(f"      è½¬æ¢ååŸå§‹å›¾åƒåæ ‡: ({orig_center_x}, {orig_center_y})")
            
            # åœ¨OCRæ£€æµ‹åˆ°çš„å¨æˆ¿æ–‡å­—ä½ç½®æ ‡è®°å¨æˆ¿
            radius = 20
            kitchen_pixels = 0
            for dy in range(-radius, radius+1):
                for dx in range(-radius, radius+1):
                    new_y = ocr_center_y + dy
                    new_x = ocr_center_x + dx
                    if (0 <= new_y < 512 and 0 <= new_x < 512 and 
                        dx*dx + dy*dy <= radius*radius):
                        floorplan[new_y, new_x] = 7  # å¨æˆ¿æ ‡ç­¾
                        kitchen_pixels += 1
            
            print(f"   âœ… åœ¨OCRæ–‡å­—ä¸­å¿ƒä½ç½®æ ‡è®°äº†{kitchen_pixels}ä¸ªåƒç´ ä¸ºå¨æˆ¿")
            print(f"   ğŸ“Š å¨æˆ¿æ–‡å­—è¯†åˆ«ç»“æœ:")
            print(f"      512x512åæ ‡: ({ocr_center_x}, {ocr_center_y})")
            print(f"      åŸå§‹å›¾åƒåæ ‡: ({orig_center_x}, {orig_center_y})")
            print(f"      è¿™å°±æ˜¯OCRç²¾ç¡®è¯†åˆ«çš„'å¨æˆ¿'ä¸¤å­—çš„ä¸­å¿ƒä½ç½®")
            
            kitchen_boxes.append({
                'center': (ocr_center_x, ocr_center_y),
                'original_center': (orig_center_x, orig_center_y),
                'bbox': (x, y, w, h),
                'text': ocr_item['text'],
                'confidence': ocr_item['confidence']
            })
    
    return floorplan, kitchen_boxes


def expand_kitchen_region_from_center(floorplan, center_x, center_y, original_shape):
    """ä»å¨æˆ¿ä¸­å¿ƒç‚¹å‘å››å‘¨å¢™å£è¾¹ç•Œå»¶ä¼¸ï¼Œç”»å‡ºæ•´ä¸ªå¨æˆ¿åŒºåŸŸ"""
    print(f"ğŸ  å¼€å§‹å¨æˆ¿åŒºåŸŸæ‰©å±•: ä¸­å¿ƒ({center_x}, {center_y})")
    
    # è·å–å›¾åƒå°ºå¯¸
    h, w = original_shape[:2]
    
    # åˆ›å»ºå¨æˆ¿æ©ç 
    kitchen_mask = np.zeros((h, w), dtype=bool)
    
    # ä½¿ç”¨åŒºåŸŸå¢é•¿ç®—æ³•ä»ä¸­å¿ƒç‚¹æ‰©å±•
    # 1. é¦–å…ˆæ ‡è®°ä¸­å¿ƒç‚¹
    if 0 <= center_y < h and 0 <= center_x < w:
        kitchen_mask[center_y, center_x] = True
    
    # 2. å‘å››ä¸ªæ–¹å‘æ‰©å±•ç›´åˆ°é‡åˆ°å¢™å£ï¼ˆé»‘è‰²åƒç´ æˆ–è¾¹ç•Œï¼‰
    directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # å³ã€å·¦ã€ä¸‹ã€ä¸Š
    
    for dx, dy in directions:
        # ä»ä¸­å¿ƒå‘æ¯ä¸ªæ–¹å‘æ‰©å±•
        current_x, current_y = center_x, center_y
        
        while True:
            current_x += dx
            current_y += dy
            
            # æ£€æŸ¥è¾¹ç•Œ
            if current_x < 0 or current_x >= w or current_y < 0 or current_y >= h:
                break
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°å¢™å£ï¼ˆå‡è®¾å¢™å£æ˜¯æ ‡ç­¾9æˆ–10ï¼‰
            if floorplan[current_y, current_x] in [9, 10]:  # å¢™å£æ ‡ç­¾
                break
            
            # æ ‡è®°ä¸ºå¨æˆ¿åŒºåŸŸ
            kitchen_mask[current_y, current_x] = True
    
    # 3. ä½¿ç”¨å½¢æ€å­¦æ“ä½œå¡«å……å°å­”æ´
    from scipy import ndimage
    kitchen_mask = ndimage.binary_fill_holes(kitchen_mask)
    
    # 4. å°†æ‰©å±•çš„åŒºåŸŸæ ‡è®°ä¸ºå¨æˆ¿
    expanded_pixels = np.sum(kitchen_mask)
    floorplan[kitchen_mask] = 7  # å¨æˆ¿æ ‡ç­¾
    
    print(f"âœ… å¨æˆ¿åŒºåŸŸæ‰©å±•å®Œæˆ: æ‰©å±•äº†{expanded_pixels}ä¸ªåƒç´ ")
    
    return kitchen_mask


def enhance_kitchen_detection(floorplan, ocr_results):
    """Enhance kitchen detection using spatial analysis and OCR results.
    
    This function uses heuristics to better identify kitchen areas:
    1. OCR text detection for explicit kitchen labels (when available)
    2. Spatial analysis - kitchens are often smaller, rectangular rooms
    3. Simple connected component analysis (without scipy dependency)
    """
    enhanced = floorplan.copy()
    h, w = enhanced.shape
    
    # First, check for OCR-based kitchen detection
    kitchen_found_by_ocr = False
    if ocr_results:
        for ocr_item in ocr_results:
            text = ocr_item['text'].lower()
            if any(keyword in text for keyword in ['å¨æˆ¿', 'kitchen', 'cook', 'çƒ¹é¥ª']):
                kitchen_found_by_ocr = True
                print(f"ğŸ³ OCRæ£€æµ‹åˆ°å¨æˆ¿æ–‡å­—: '{ocr_item['text']}'")
                break
    
    if not kitchen_found_by_ocr:
        print("ğŸ“ OCRæœªæ£€æµ‹åˆ°å¨æˆ¿æ–‡å­—ï¼Œä½¿ç”¨ç©ºé—´åˆ†ææ–¹æ³•...")
    
    # Find regions labeled as living/dining (class 3)
    living_dining_mask = (enhanced == 3)
    
    if np.sum(living_dining_mask) == 0:
        print("âŒ æœªå‘ç°å®¢å…/é¤å…/å¨æˆ¿åŒºåŸŸ")
        return enhanced
    
    try:
        # Use simple connected component analysis
        labeled_regions, num_regions = simple_connected_components(living_dining_mask)
        
        print(f"ğŸ” å‘ç° {num_regions} ä¸ªå®¢å…/é¤å…/å¨æˆ¿åŒºåŸŸ")
        
        region_stats = []
        
        for region_id in range(1, num_regions + 1):
            region_mask = (labeled_regions == region_id)
            region_area = np.sum(region_mask)
            
            # Get bounding box of this region
            region_coords = np.where(region_mask)
            if len(region_coords[0]) == 0:
                continue
                
            min_y, max_y = np.min(region_coords[0]), np.max(region_coords[0])
            min_x, max_x = np.min(region_coords[1]), np.max(region_coords[1])
            region_height = max_y - min_y + 1
            region_width = max_x - min_x + 1
            
            # Calculate various metrics
            aspect_ratio = max(region_width, region_height) / min(region_width, region_height)
            density = region_area / (region_width * region_height)
            relative_area = region_area / (h * w)
            
            # Check if any OCR results suggest this is a kitchen
            has_kitchen_text = False
            if ocr_results:
                for ocr_item in ocr_results:
                    if any(keyword in ocr_item['text'].lower() for keyword in ['å¨æˆ¿', 'kitchen', 'cook', 'çƒ¹é¥ª']):
                        ocr_x, ocr_y, ocr_w, ocr_h = ocr_item['bbox']
                        # Check if OCR text is within this region
                        if (min_x <= ocr_x + ocr_w/2 <= max_x and min_y <= ocr_y + ocr_h/2 <= max_y):
                            has_kitchen_text = True
                            break
            
            region_stats.append({
                'id': region_id,
                'mask': region_mask,
                'area': region_area,
                'relative_area': relative_area,
                'aspect_ratio': aspect_ratio,
                'density': density,
                'width': region_width,
                'height': region_height,
                'has_kitchen_text': has_kitchen_text,
                'bbox': (min_x, min_y, max_x, max_y)
            })
        
        # Sort regions by area (smallest first - kitchens are often smaller)
        region_stats.sort(key=lambda x: x['relative_area'])
        
        # Enhanced heuristics for kitchen detection
        kitchen_assigned = False
        for i, stats in enumerate(region_stats):
            print(f"   åŒºåŸŸ{stats['id']}: é¢ç§¯={stats['relative_area']:.3f}, é•¿å®½æ¯”={stats['aspect_ratio']:.2f}, å¯†åº¦={stats['density']:.2f}")
            
            is_kitchen = False
            reasons = []
            
            # Rule 1: Explicit OCR detection
            if stats['has_kitchen_text']:
                is_kitchen = True
                reasons.append("OCRæ£€æµ‹åˆ°å¨æˆ¿æ–‡å­—")
            
            # Rule 2: Small area + reasonable shape (only assign one kitchen)
            elif (not kitchen_assigned and 
                  stats['relative_area'] < 0.15 and  # Smaller than 15% of total area
                  1.0 < stats['aspect_ratio'] < 4.0 and  # Not too elongated
                  stats['density'] > 0.5):  # Good density
                is_kitchen = True
                reasons.append("å°é¢ç§¯+åˆç†å½¢çŠ¶")
            
            # Rule 3: Very small compact area (only assign one kitchen)
            elif (not kitchen_assigned and
                  stats['relative_area'] < 0.08 and  # Very small area
                  stats['aspect_ratio'] < 3.0 and
                  stats['density'] > 0.6):
                is_kitchen = True
                reasons.append("ç´§å‡‘å‹å¨æˆ¿")
            
            # Rule 4: If multiple regions, the smallest reasonable one might be kitchen
            elif (not kitchen_assigned and
                  len(region_stats) > 1 and i == 0 and  # Smallest region
                  stats['relative_area'] < 0.2 and
                  stats['aspect_ratio'] < 5.0 and
                  stats['density'] > 0.4):
                is_kitchen = True
                reasons.append("å¤šåŒºåŸŸä¸­æœ€å°çš„åˆç†åŒºåŸŸ")
            
            if is_kitchen:
                enhanced[stats['mask']] = 7  # Kitchen class
                print(f"   âœ… è¯†åˆ«ä¸ºå¨æˆ¿: {', '.join(reasons)}")
                kitchen_assigned = True
            else:
                print(f"   âŒ ä¿æŒä¸ºå®¢å…/é¤å…")
                
        if not kitchen_assigned:
            print("âš ï¸ æœªèƒ½è‡ªåŠ¨è¯†åˆ«å¨æˆ¿ï¼Œæ‰€æœ‰åŒºåŸŸä¿æŒä¸ºå®¢å…/é¤å…")
                
    except Exception as e:
        print(f"âš ï¸ ç©ºé—´åˆ†æå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    return enhanced

def ind2rgb(ind_im, enable_closet=True):
        # Use the appropriate color map based on closet setting
        if enable_closet:
            color_map = floorplan_fuse_map_figure
        else:
            # Create a modified map without closet
            color_map = floorplan_fuse_map_figure.copy()
            color_map[1] = color_map[3]  # Map closet to living room color
        
        rgb_im = np.zeros((ind_im.shape[0], ind_im.shape[1], 3))

        for i, rgb in color_map.items():
                rgb_im[(ind_im==i)] = rgb

        return rgb_im

def main(args):
        enable_closet = not args.disable_closet
        set_closet_enabled(enable_closet)

        # load input
        im = imread(args.im_path, mode='RGB')
        # Keep original size for better OCR
        original_im = im.copy()
        
        print(f"ğŸ–¼ï¸ å›¾åƒå¤„ç†æµç¨‹è¯¦ç»†åˆ†æ:")
        print(f"   ğŸ“ åŸå§‹å›¾åƒå°ºå¯¸: {original_im.shape[1]} x {original_im.shape[0]} (å®½xé«˜)")
        
        # Resize image for network inference
        im = imresize(im, (512, 512))
        print(f"   ğŸ”„ ç¥ç»ç½‘ç»œè¾“å…¥: 512 x 512 (å›ºå®šå°ºå¯¸)")
        print(f"   ğŸ’¡ ä¸ºä»€ä¹ˆè¦è½¬æ¢åˆ°512x512ï¼Ÿ")
        print(f"      - DeepFloorplanç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„æ˜¯512x512è¾“å…¥")
        print(f"      - æ‰€æœ‰çš„ç¥ç»ç½‘ç»œæ¨ç†éƒ½åœ¨512x512åæ ‡ç³»ä¸­è¿›è¡Œ")
        print(f"      - æœ€ç»ˆç»“æœéœ€è¦è½¬æ¢å›åŸå§‹å›¾åƒå°ºå¯¸ç”¨äºæ˜¾ç¤º")
        
        # For OCR, use larger, enhanced image
        from PIL import Image, ImageEnhance
        ocr_img = Image.fromarray(original_im)
        # Enlarge for better OCR
        ocr_img = ocr_img.resize((ocr_img.width * 2, ocr_img.height * 2), Image.LANCZOS)
        print(f"   ğŸ” OCRå¤„ç†å›¾åƒ: {ocr_img.width} x {ocr_img.height} (æ”¾å¤§2å€)")
        print(f"   ğŸ’¡ ä¸ºä»€ä¹ˆOCRè¦æ”¾å¤§2å€ï¼Ÿ")
        print(f"      - æé«˜OCRè¯†åˆ«å°æ–‡å­—çš„å‡†ç¡®æ€§")
        print(f"      - å¢å¼ºæ–‡å­—çš„æ¸…æ™°åº¦å’Œå¯¹æ¯”åº¦")
        
        # Enhance contrast
        enhancer = ImageEnhance.Contrast(ocr_img)
        ocr_img = enhancer.enhance(2.5)
        # Enhance sharpness
        enhancer = ImageEnhance.Sharpness(ocr_img)
        ocr_img = enhancer.enhance(2.0)
        ocr_im = np.array(ocr_img)
        
        print(f"   ğŸ“Š åæ ‡è½¬æ¢å…³ç³»:")
        print(f"      åŸå§‹å›¾åƒ -> 512x512: ç¼©æ”¾æ¯”ä¾‹ X={512/original_im.shape[1]:.3f}, Y={512/original_im.shape[0]:.3f}")
        print(f"      512x512 -> åŸå§‹å›¾åƒ: ç¼©æ”¾æ¯”ä¾‹ X={original_im.shape[1]/512:.3f}, Y={original_im.shape[0]/512:.3f}")
        print(f"      âš ï¸ å…³é”®: Xæ¯”ä¾‹={original_im.shape[1]/512:.3f} å°±æ˜¯æ‚¨çœ‹åˆ°çš„1.131!")
        print(f"      ğŸ“ è®¡ç®—: {original_im.shape[1]} Ã· 512 = {original_im.shape[1]/512:.3f}")
        
        # Extract textual room labels using OCR with enhanced image
        ocr_results = extract_room_text(ocr_im)
        # Scale OCR bounding boxes to match segmentation size (512x512)
        if ocr_results:
                scale_x = im.shape[1] / ocr_im.shape[1]
                scale_y = im.shape[0] / ocr_im.shape[0]
                print(f"   ğŸ”„ OCRåæ ‡è½¬æ¢åˆ°512x512:")
                print(f"      OCRå›¾åƒ({ocr_im.shape[1]}x{ocr_im.shape[0]}) -> 512x512")
                print(f"      è½¬æ¢æ¯”ä¾‹: X={scale_x:.3f}, Y={scale_y:.3f}")
                for item in ocr_results:
                        x, y, w, h = item['bbox']
                        x = int(x * scale_x)
                        y = int(y * scale_y)
                        w = int(w * scale_x)
                        h = int(h * scale_y)
                        item['bbox'] = (x, y, w, h)
        
        # Convert to float and normalize for network inference
        im = im.astype(np.float32) / 255.

        # create tensorflow session with CPU configuration
        config = tf.ConfigProto(
                device_count={'GPU': 0},  # Disable GPU
                allow_soft_placement=True,
                log_device_placement=False
        )
        with tf.Session(config=config) as sess:
                
                # initialize
                sess.run(tf.group(tf.global_variables_initializer(),
                                        tf.local_variables_initializer()))

                # restore pretrained model
                saver = tf.train.import_meta_graph('./pretrained/pretrained_r3d.meta')
                saver.restore(sess, './pretrained/pretrained_r3d')

                # get default graph
                graph = tf.get_default_graph()

                # restore inputs & outpus tensor
                x = graph.get_tensor_by_name('inputs:0')
                room_type_logit = graph.get_tensor_by_name('Cast:0')
                room_boundary_logit = graph.get_tensor_by_name('Cast_1:0')

                # infer results
                [room_type, room_boundary] = sess.run([room_type_logit, room_boundary_logit],\
                                                                feed_dict={x:im.reshape(1,512,512,3)})
                room_type, room_boundary = np.squeeze(room_type), np.squeeze(room_boundary)

                # merge results
                floorplan = room_type.copy()
                floorplan[room_boundary==1] = 9
                floorplan[room_boundary==2] = 10
                
                # ğŸ¯ åº”ç”¨ç²¾ç¡®å¨æˆ¿åæ ‡è½¬æ¢ - ç›´æ¥ä½¿ç”¨OCRæ£€æµ‹çš„å¨æˆ¿æ–‡å­—ä½ç½®
                floorplan, kitchen_boxes = apply_precise_kitchen_coordinates(floorplan, ocr_results, original_im.shape[:2])
                
                # ğŸ  ä»å¨æˆ¿ä¸­å¿ƒå‘å››å‘¨æ‰©å±•åˆ°å¢™å£è¾¹ç•Œ
                if kitchen_boxes:
                    for kitchen_info in kitchen_boxes:
                        center_x, center_y = kitchen_info['center']
                        # å°†512x512åæ ‡è½¬æ¢ä¸ºåŸå§‹å›¾åƒåæ ‡è¿›è¡ŒåŒºåŸŸæ‰©å±•
                        orig_center_x = int(center_x * original_im.shape[1] / 512)
                        orig_center_y = int(center_y * original_im.shape[0] / 512)
                        
                        # åˆ›å»ºåŸå§‹å°ºå¯¸çš„floorplanç”¨äºåŒºåŸŸæ‰©å±•
                        original_h, original_w = original_im.shape[:2]
                        floorplan_full_size = cv2.resize(floorplan.astype(np.uint8), (original_w, original_h), interpolation=cv2.INTER_NEAREST)
                        
                        print(f"ğŸ  å¼€å§‹å¨æˆ¿åŒºåŸŸæ‰©å±•: ä»({orig_center_x}, {orig_center_y})å‘å››å‘¨å¢™å£å»¶ä¼¸")
                        kitchen_mask = expand_kitchen_region_from_center(floorplan_full_size, orig_center_x, orig_center_y, original_im.shape)
                        
                        # å°†æ‰©å±•ç»“æœç¼©æ”¾å›512x512ç”¨äºåç»­å¤„ç†
                        kitchen_mask_512 = cv2.resize(kitchen_mask.astype(np.uint8), (512, 512), interpolation=cv2.INTER_NEAREST)
                        floorplan[kitchen_mask_512 > 0] = 7  # å°†æ‰©å±•åŒºåŸŸæ ‡è®°ä¸ºå¨æˆ¿
                
                # Use OCR labels to refine room categories
                floorplan = fuse_ocr_and_segmentation(floorplan, ocr_results)
                # Enhance kitchen detection (now with precise coordinates)
                floorplan = enhance_kitchen_detection(floorplan, ocr_results)
                if not enable_closet:
                        floorplan[floorplan==1] = 0
                floorplan_rgb = ind2rgb(floorplan, enable_closet)
                
                # ğŸ¯ æ·»åŠ å¨æˆ¿ä½ç½®çš„çº¢è‰²æ ‡è®°å’Œåæ ‡ç½‘æ ¼
                if kitchen_boxes:
                    # è½¬æ¢ä¸ºåŸå§‹å›¾åƒå°ºå¯¸
                    original_h, original_w = original_im.shape[:2]
                    floorplan_original_size = cv2.resize(floorplan_rgb, (original_w, original_h), interpolation=cv2.INTER_NEAREST)
                    
                    # æ·»åŠ æ›´æ˜æ˜¾çš„åæ ‡ç½‘æ ¼ (æ¯25åƒç´ ä¸€æ¡ç»†çº¿ï¼Œæ¯100åƒç´ ä¸€æ¡ç²—çº¿)
                    for x in range(0, original_w, 25):
                        thickness = 2 if x % 100 == 0 else 1
                        color = (0, 0, 255) if x % 100 == 0 else (128, 128, 128)  # çº¢è‰²ä¸»çº¿ï¼Œç°è‰²ç»†çº¿
                        cv2.line(floorplan_original_size, (x, 0), (x, original_h), color, thickness)
                    for y in range(0, original_h, 25):
                        thickness = 2 if y % 100 == 0 else 1
                        color = (0, 0, 255) if y % 100 == 0 else (128, 128, 128)  # çº¢è‰²ä¸»çº¿ï¼Œç°è‰²ç»†çº¿
                        cv2.line(floorplan_original_size, (0, y), (original_w, y), color, thickness)
                    
                    # æ·»åŠ åæ ‡æ ‡æ³¨ (æ¯50åƒç´ )
                    for x in range(0, original_w, 50):
                        cv2.putText(floorplan_original_size, str(x), (x+2, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                    for y in range(0, original_h, 50):
                        cv2.putText(floorplan_original_size, str(y), (5, y+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                    
                    # æ ‡è®°OCRè¯†åˆ«çš„å¨æˆ¿æ–‡å­—ä¸­å¿ƒä½ç½®
                    for kitchen_info in kitchen_boxes:
                        # OCRè¯†åˆ«çš„å¨æˆ¿æ–‡å­—ä¸­å¿ƒä½ç½®ï¼ˆç»¿è‰² - è¿™å°±æ˜¯"å¨æˆ¿"ä¸¤å­—çš„ç²¾ç¡®ä¸­å¿ƒï¼‰
                        ocr_x, ocr_y = kitchen_info['original_center']
                        cv2.rectangle(floorplan_original_size, 
                                    (ocr_x-30, ocr_y-30), 
                                    (ocr_x+30, ocr_y+30), 
                                    (0, 255, 0), 4)
                        cv2.circle(floorplan_original_size, (ocr_x, ocr_y), 8, (0, 255, 0), -1)
                        
                        # OCRä½ç½®æ ‡æ³¨
                        ocr_text = f"OCRå¨æˆ¿æ–‡å­—ä¸­å¿ƒ({ocr_x},{ocr_y})"
                        cv2.putText(floorplan_original_size, ocr_text, 
                                  (ocr_x+35, ocr_y-20), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                        
                        # æ·»åŠ ç™½è‰²èƒŒæ™¯
                        text_size = cv2.getTextSize(ocr_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
                        cv2.rectangle(floorplan_original_size,
                                    (ocr_x+33, ocr_y-35),
                                    (ocr_x+37+text_size[0], ocr_y-15),
                                    (255, 255, 255), -1)
                        cv2.putText(floorplan_original_size, ocr_text, 
                                  (ocr_x+35, ocr_y-20), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                        
                        print(f"ğŸ¯ OCRè¯†åˆ«çš„å¨æˆ¿æ–‡å­—ä¸­å¿ƒ: ç»¿è‰²æ¡†({ocr_x}, {ocr_y})")
                        print(f"ï¿½ è¿™æ˜¯'å¨æˆ¿'ä¸¤ä¸ªå­—çš„ç²¾ç¡®ä¸­å¿ƒä½ç½®")
                        
                        # æ·»åŠ å›¾ä¾‹è¯´æ˜
                        legend_y = 30
                        cv2.putText(floorplan_original_size, "ç»¿è‰²=OCRè¯†åˆ«å¨æˆ¿æ–‡å­—ä¸­å¿ƒ", 
                                  (10, legend_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                        cv2.putText(floorplan_original_size, "è¿™æ˜¯'å¨æˆ¿'ä¸¤å­—çš„ç²¾ç¡®ä½ç½®", 
                                  (10, legend_y + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    
                    # ä¿å­˜å¸¦æ ‡è®°çš„ç»“æœ
                    marked_filename = FLAGS.im_path.replace('.jpg', '_marked.png').replace('.png', '_marked.png')
                    imsave(marked_filename, floorplan_original_size)
                    print(f"âœ… å¸¦å¨æˆ¿æ ‡è®°çš„ç»“æœå·²ä¿å­˜: {marked_filename}")
                else:
                    # æ²¡æœ‰å¨æˆ¿æ—¶ï¼Œä¹Ÿæ·»åŠ åæ ‡ç½‘æ ¼ä¾¿äºåˆ†æ
                    original_h, original_w = original_im.shape[:2]
                    floorplan_original_size = cv2.resize(floorplan_rgb, (original_w, original_h), interpolation=cv2.INTER_NEAREST)
                    
                    # æ·»åŠ åæ ‡ç½‘æ ¼
                    for x in range(0, original_w, 25):
                        thickness = 2 if x % 100 == 0 else 1
                        color = (0, 0, 255) if x % 100 == 0 else (128, 128, 128)
                        cv2.line(floorplan_original_size, (x, 0), (x, original_h), color, thickness)
                    for y in range(0, original_h, 25):
                        thickness = 2 if y % 100 == 0 else 1
                        color = (0, 0, 255) if y % 100 == 0 else (128, 128, 128)
                        cv2.line(floorplan_original_size, (0, y), (original_w, y), color, thickness)
                    
                    # æ·»åŠ åæ ‡æ ‡æ³¨
                    for x in range(0, original_w, 50):
                        cv2.putText(floorplan_original_size, str(x), (x+2, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                    for y in range(0, original_h, 50):
                        cv2.putText(floorplan_original_size, str(y), (5, y+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)

                # plot results with coordinate axes
                plt.figure(figsize=(18, 8))
                plt.subplot(131)
                plt.imshow(im)
                plt.title('åŸå§‹å›¾ç‰‡')
                plt.axis('on')  # æ˜¾ç¤ºåæ ‡è½´
                plt.grid(True, alpha=0.3)
                
                plt.subplot(132)
                plt.imshow(floorplan_rgb/255.)
                plt.title('æˆ·å‹åˆ†æç»“æœ (ç»¿è‰²=å¨æˆ¿)')
                plt.axis('on')  # æ˜¾ç¤ºåæ ‡è½´
                plt.grid(True, alpha=0.3)
                
                # ç¬¬ä¸‰ä¸ªå­å›¾ï¼šæ˜¾ç¤ºå¸¦æ ‡è®°çš„ç»“æœ
                if kitchen_boxes:
                    plt.subplot(133)
                    plt.imshow(floorplan_original_size)
                    plt.title('å¨æˆ¿æ ‡è®°ç»“æœ (çº¢è‰²=å¨æˆ¿ä½ç½®)')
                    plt.axis('on')  # æ˜¾ç¤ºåæ ‡è½´
                    plt.grid(True, alpha=0.3)
                    
                    # åœ¨å›¾ä¸Šæ ‡æ³¨å¨æˆ¿åæ ‡
                    for kitchen_info in kitchen_boxes:
                        orig_x, orig_y = kitchen_info['original_center']
                        plt.plot(orig_x, orig_y, 'ro', markersize=8, label=f"å¨æˆ¿({orig_x},{orig_y})")
                        plt.annotate(f"{kitchen_info['text']}\n({orig_x},{orig_y})", 
                                   (orig_x, orig_y), 
                                   xytext=(10, -10), 
                                   textcoords='offset points',
                                   bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.7),
                                   fontsize=10, color='white')
                    plt.legend()
                
                # Save result
                output_name = os.path.basename(args.im_path).split('.')[0] + '_result.png'
                plt.savefig(output_name, dpi=300, bbox_inches='tight')
                print(f"ğŸ“¸ ç»“æœå·²ä¿å­˜: {output_name}")
                
                plt.show()

if __name__ == '__main__':
        FLAGS, unparsed = parser.parse_known_args()
        main(FLAGS)
