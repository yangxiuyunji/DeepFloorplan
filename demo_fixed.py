#!/usr/bin/env python3
"""
ä¿®æ”¹demo.pyä»¥ç›´æ¥ä¿å­˜RGBå›¾åƒï¼Œé¿å…matplotlibæ’å€¼
"""
import os
import argparse
import numpy as np
import tensorflow.compat.v1 as tf
import matplotlib.pyplot as plt
import matplotlib
from PIL import Image
import cv2

# é…ç½®TensorFlowæ—¥å¿—çº§åˆ«ï¼Œå®Œå…¨é™éŸ³å†—é•¿è¾“å‡º
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # åªæ˜¾ç¤ºé”™è¯¯
import warnings
warnings.filterwarnings('ignore')

tf.logging.set_verbosity(tf.logging.ERROR)  # å‡å°‘TensorFlowæ—¥å¿—

# Configure Chinese font support for matplotlib
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# Disable TF 2.x behavior for compatibility
tf.disable_v2_behavior()

# OCR utilities - use enhanced version with PaddleOCR support
from utils.ocr_enhanced import extract_room_text, fuse_ocr_and_segmentation, set_closet_enabled
from utils.rgb_ind_convertor import floorplan_fuse_map, floorplan_fuse_map_figure

def draw_room_regions_with_info(original_image, enhanced_result, ocr_results, output_path):
    """
    åœ¨æˆ¿é—´åˆ†å‰²ç»“æœä¸Šç»˜åˆ¶æˆ¿é—´åŒºåŸŸä¿¡æ¯ï¼ˆåæ ‡ã€é¢ç§¯ç­‰ï¼‰- ä¿®å¤è¿‡åº¦åˆ†å‰²
    
    Args:
        original_image: åŸå§‹å›¾åƒ (numpy array)
        enhanced_result: æˆ¿é—´åˆ†å‰²ç»“æœ (numpy array)
        ocr_results: OCRæ£€æµ‹ç»“æœåˆ—è¡¨
        output_path: è¾“å‡ºå›¾åƒè·¯å¾„
    """
    import cv2
    from scipy import ndimage
    from PIL import Image, ImageDraw, ImageFont
    import numpy as np
    
    # è½¬æ¢ä¸ºPILå›¾åƒä»¥ä¾¿å¤„ç†ä¸­æ–‡
    pil_image = Image.fromarray(original_image)
    draw = ImageDraw.Draw(pil_image)
    
    # å®šä¹‰æˆ¿é—´æ ‡ç­¾å¯¹åº”çš„é¢œè‰²å’Œåç§° (RGBæ ¼å¼ï¼ŒPILä½¿ç”¨RGB)
    room_info = {
        1: {'name': 'æœªåˆ†ç±»', 'color': (128, 128, 128), 'emoji': 'ğŸ '},
        2: {'name': 'å«ç”Ÿé—´', 'color': (0, 0, 255), 'emoji': 'ğŸš¿'},
        3: {'name': 'å®¢å…/é¤å…', 'color': (0, 255, 0), 'emoji': 'ğŸ›‹ï¸'},
        4: {'name': 'å§å®¤', 'color': (255, 255, 0), 'emoji': 'ğŸ›ï¸'},
        5: {'name': 'ç„å…³/å¤§å…', 'color': (128, 0, 128), 'emoji': 'ğŸšª'},
        6: {'name': 'é˜³å°', 'color': (255, 0, 255), 'emoji': 'ğŸŒ¿'},
        7: {'name': 'å¨æˆ¿', 'color': (255, 165, 0), 'emoji': 'ğŸ³'},
    }
    
    # å°è¯•åŠ è½½ä¸­æ–‡å­—ä½“
    try:
        # Windowsç³»ç»Ÿå­—ä½“è·¯å¾„
        font_path = "C:/Windows/Fonts/simhei.ttf"  # é»‘ä½“
        font = ImageFont.truetype(font_path, 16)
        small_font = ImageFont.truetype(font_path, 14)
    except:
        try:
            # å¤‡é€‰å­—ä½“
            font_path = "C:/Windows/Fonts/msyh.ttc"  # å¾®è½¯é›…é»‘
            font = ImageFont.truetype(font_path, 16)
            small_font = ImageFont.truetype(font_path, 14)
        except:
            # ä½¿ç”¨é»˜è®¤å­—ä½“
            font = ImageFont.load_default()
            small_font = ImageFont.load_default()
    
    print(f"\nğŸ¯ ç»˜åˆ¶æˆ¿é—´åŒºåŸŸå’Œä¿¡æ¯æ ‡æ³¨:")
    
    # è·å–å›¾åƒå°ºå¯¸
    h_orig, w_orig = original_image.shape[:2]
    
    # ç¼©æ”¾åˆ†å‰²ç»“æœåˆ°åŸå§‹å›¾åƒå°ºå¯¸
    if enhanced_result.shape != (h_orig, w_orig):
        from scipy.ndimage import zoom
        scale_y = h_orig / enhanced_result.shape[0]
        scale_x = w_orig / enhanced_result.shape[1]
        enhanced_result = zoom(enhanced_result, (scale_y, scale_x), order=0)
    
    # ğŸ”§ åˆå¹¶åŒç±»å‹æˆ¿é—´åŒºåŸŸçš„æ–°ç®—æ³•
    def merge_room_regions(mask, room_label):
        """æ™ºèƒ½åˆå¹¶åŒç±»å‹æˆ¿é—´åŒºåŸŸï¼Œå‡å°‘è¿‡åº¦åˆ†å‰²"""
        if not np.any(mask):
            return []
        
        # æ ¹æ®æˆ¿é—´ç±»å‹é€‰æ‹©ä¸åŒçš„åˆå¹¶ç­–ç•¥
        if room_label == 2:  # å«ç”Ÿé—´ - é€šå¸¸è¾ƒå°ï¼Œä½¿ç”¨ä¿å®ˆåˆå¹¶
            structure = np.ones((3, 3))
            iterations = 1
            min_area = 300  # è¾ƒå°çš„æœ€å°é¢ç§¯è¦æ±‚
        elif room_label in [3, 4]:  # å®¢å…ã€å§å®¤ - é€šå¸¸è¾ƒå¤§ï¼Œä½¿ç”¨æ¿€è¿›åˆå¹¶
            structure = np.ones((7, 7))
            iterations = 3
            min_area = 2000  # è¾ƒå¤§çš„æœ€å°é¢ç§¯è¦æ±‚
        elif room_label == 7:  # å¨æˆ¿ - ä¸­ç­‰å¤§å°
            structure = np.ones((5, 5))
            iterations = 2
            min_area = 800
        else:  # å…¶ä»–æˆ¿é—´ç±»å‹
            structure = np.ones((4, 4))
            iterations = 2
            min_area = 500
        
        # å½¢æ€å­¦é—­è¿ç®—ï¼Œè¿æ¥nearbyçš„åŒç±»å‹åŒºåŸŸ
        closed_mask = ndimage.binary_closing(mask, structure=structure, iterations=iterations)
        
        # å¡«å……å°å­”æ´
        filled_mask = ndimage.binary_fill_holes(closed_mask)
        
        # å†æ¬¡è¿›è¡Œå¼€è¿ç®—ï¼Œå¹³æ»‘è¾¹ç•Œ
        opened_mask = ndimage.binary_opening(filled_mask, structure=np.ones((2, 2)))
        
        # è¿é€šç»„ä»¶åˆ†æ
        labeled_mask, num_features = ndimage.label(opened_mask)
        
        valid_regions = []
        for region_id in range(1, num_features + 1):
            region_mask = (labeled_mask == region_id)
            
            # æ£€æŸ¥åŒºåŸŸå¤§å°
            area = np.sum(region_mask)
            if area < min_area:
                continue
            
            # æ£€æŸ¥ä¸åŸå§‹maskçš„é‡å åº¦
            overlap = np.sum(mask & region_mask)
            overlap_ratio = overlap / area
            
            # åªä¿ç•™é‡å åº¦è¶³å¤Ÿé«˜çš„åŒºåŸŸ
            if overlap_ratio > 0.4:  # è‡³å°‘40%é‡å 
                valid_regions.append(region_mask)
        
        return valid_regions
    
    # ç»Ÿè®¡åˆå¹¶åçš„æˆ¿é—´åŒºåŸŸ
    room_regions = []
    room_count = {}
    
    # éå†æ¯ç§æˆ¿é—´ç±»å‹
    for room_label, info in room_info.items():
        room_name = info['name']
        room_color = info['color']
        room_emoji = info['emoji']
        
        # æ‰¾åˆ°è¯¥ç±»å‹çš„æ‰€æœ‰åƒç´ 
        mask = (enhanced_result == room_label)
        if not np.any(mask):
            continue
        
        # ä½¿ç”¨æ™ºèƒ½åˆå¹¶ç®—æ³•
        merged_regions = merge_room_regions(mask, room_label)
        
        for region_mask in merged_regions:
            # è®¡ç®—è¾¹ç•Œæ¡†
            y_coords, x_coords = np.where(region_mask)
            if len(y_coords) == 0:
                continue
                
            x1, y1 = int(np.min(x_coords)), int(np.min(y_coords))
            x2, y2 = int(np.max(x_coords)), int(np.max(y_coords))
            w, h = x2 - x1 + 1, y2 - y1 + 1
            area = np.sum(region_mask)
            
            room_regions.append({
                'room_type': room_name,
                'room_label': room_label,
                'color': room_color,
                'emoji': room_emoji,
                'bbox': (x1, y1, x2, y2),
                'area': area,
                'center': ((x1 + x2) // 2, (y1 + y2) // 2)
            })
            
            # ç»Ÿè®¡æˆ¿é—´æ•°é‡
            room_count[room_name] = room_count.get(room_name, 0) + 1
    
    print(f"ğŸ“¸ å…±è¯†åˆ«å‡º {len(room_regions)} ä¸ªåˆç†çš„æˆ¿é—´åŒºåŸŸ")
    
    # ç»˜åˆ¶æ¯ä¸ªåŒºåŸŸ
    for i, region in enumerate(room_regions):
        room_type = region['room_type']
        emoji = region['emoji']
        color = region['color']
        x1, y1, x2, y2 = region['bbox']
        area = region['area']
        w, h = x2 - x1, y2 - y1
        center_x, center_y = region['center']
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
        
        # åˆ›å»ºä¿¡æ¯æ–‡æœ¬
        info_lines = [
            f"{emoji} {room_type}",
            f"åæ ‡èŒƒå›´: ({x1}, {y1}) -> ({x2}, {y2})",
            f"å°ºå¯¸: {w} Ã— {h} åƒç´ ",
            f"é¢ç§¯: {area} åƒç´ Â²",
            f"ä¸­å¿ƒç‚¹: ({center_x}, {center_y})"
        ]
        
        # è®¡ç®—ä¿¡æ¯æ¡†ä½ç½® - æ™ºèƒ½é¿è®©
        info_x = min(x2 + 10, w_orig - 220)
        info_y = y1
        
        line_height = 18
        info_box_width = 200
        info_box_height = len(info_lines) * line_height + 12
        
        # ç¡®ä¿ä¿¡æ¯æ¡†ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
        if info_y + info_box_height > h_orig:
            info_y = max(0, h_orig - info_box_height)
        if info_x < 0:
            info_x = max(0, x1 - info_box_width - 10)
        
        # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯æ¡†
        bbox_bg = Image.new('RGBA', (info_box_width, info_box_height), (0, 0, 0, 150))
        pil_image.paste(bbox_bg, (info_x, info_y), bbox_bg)
        
        # ç»˜åˆ¶è¾¹æ¡†
        draw.rectangle([info_x, info_y, info_x + info_box_width, info_y + info_box_height], 
                      outline=color, width=2)
        
        # ç»˜åˆ¶æ–‡æœ¬ä¿¡æ¯
        for idx, line in enumerate(info_lines):
            text_y = info_y + 8 + idx * line_height
            draw.text((info_x + 8, text_y), line, fill=(255, 255, 255), font=small_font)
        
        # åœ¨æˆ¿é—´ä¸­å¿ƒç»˜åˆ¶åºå·
        draw.text((center_x - 8, center_y - 8), str(i+1), fill=(255, 255, 255), font=font)
        
        # æ§åˆ¶å°è¾“å‡º
        print(f"   {i+1}. {emoji} {room_type}")
        print(f"      ğŸ“ åæ ‡èŒƒå›´: ({x1}, {y1}) -> ({x2}, {y2})")
        print(f"      ğŸ“ å°ºå¯¸: {w} Ã— {h} åƒç´ ")
        print(f"      ğŸ“ é¢ç§¯: {area} åƒç´ Â²")
        print(f"      ğŸ¯ ä¸­å¿ƒç‚¹: ({center_x}, {center_y})")
        print()
    
    # æ‰“å°æˆ¿é—´ç»Ÿè®¡
    print(f"ğŸ“Š æˆ¿é—´ç±»å‹ç»Ÿè®¡:")
    for room_type, count in room_count.items():
        emoji = next(info['emoji'] for info in room_info.values() if info['name'] == room_type)
        print(f"   {emoji} {room_type}: {count} ä¸ª")
    
    # ä¿å­˜å›¾åƒ
    pil_image.save(output_path)
    print(f"ğŸ“¸ æˆ¿é—´åŒºåŸŸæ ‡æ³¨å›¾åƒå·²ä¿å­˜: {output_path}")
    h, w = enhanced_result.shape[:2]
    room_regions = []
    
    # åˆ†ææ¯ä¸ªæˆ¿é—´æ ‡ç­¾
    for label in range(1, 8):  # 1-7 å¯¹åº”ä¸åŒæˆ¿é—´ç±»å‹
        if label in [9, 10]:  # è·³è¿‡å¢™ä½“å’Œé—¨çª—
            continue
            
        # æ‰¾åˆ°è¯¥æ ‡ç­¾çš„æ‰€æœ‰åŒºåŸŸ
        mask = (enhanced_result == label)
        if not np.any(mask):
            continue
            
        # ä½¿ç”¨è¿é€šç»„ä»¶åˆ†ææ‰¾åˆ°ç‹¬ç«‹çš„æˆ¿é—´åŒºåŸŸ
        labeled_regions, num_regions = ndimage.label(mask)
        
        for region_id in range(1, num_regions + 1):
            region_mask = (labeled_regions == region_id)
            
            # è®¡ç®—åŒºåŸŸå±æ€§
            coords = np.where(region_mask)
            if len(coords[0]) == 0:
                continue
                
            min_y, max_y = coords[0].min(), coords[0].max()
            min_x, max_x = coords[1].min(), coords[1].max()
            
            # è®¡ç®—åŒºåŸŸä¿¡æ¯
            area_pixels = np.sum(region_mask)
            width = max_x - min_x + 1
            height = max_y - min_y + 1
            
            # è·³è¿‡è¿‡å°çš„åŒºåŸŸ
            if area_pixels < 100:  # è‡³å°‘100åƒç´ 
                continue
                
            # è®¡ç®—ä¸­å¿ƒç‚¹
            center_y = int(np.mean(coords[0]))
            center_x = int(np.mean(coords[1]))
            
            room_regions.append({
                'label': label,
                'name': room_info[label]['name'],
                'color': room_info[label]['color'],
                'emoji': room_info[label]['emoji'],
                'bbox': (min_x, min_y, max_x, max_y),
                'center': (center_x, center_y),
                'area': area_pixels,
                'width': width,
                'height': height
            })
    
    print(f"ğŸ“¸ å…±è¯†åˆ«å‡º {len(room_regions)} ä¸ªæˆ¿é—´åŒºåŸŸ")
    
    # åœ¨å›¾åƒä¸Šç»˜åˆ¶æˆ¿é—´ä¿¡æ¯
    for i, room in enumerate(room_regions):
        label = room['label']
        name = room['name']
        color = room['color']
        emoji = room['emoji']
        min_x, min_y, max_x, max_y = room['bbox']
        center_x, center_y = room['center']
        area = room['area']
        width = room['width']
        height = room['height']
        
        # ä½¿ç”¨PILç»˜åˆ¶è¾¹ç•Œæ¡†
        draw.rectangle([min_x, min_y, max_x, max_y], outline=color, width=3)
        
        # å‡†å¤‡æˆ¿é—´ä¿¡æ¯æ–‡æœ¬
        info_lines = [
            f"{name}",
            f"åæ ‡: ({min_x},{min_y})-({max_x},{max_y})",
            f"å°ºå¯¸: {width}x{height}px",
            f"é¢ç§¯: {area}pxÂ²"
        ]
        
        # è®¡ç®—æ–‡æœ¬æ¡†å¤§å°
        line_height = 20
        max_text_width = 0
        for line in info_lines:
            bbox_text = draw.textbbox((0, 0), line, font=small_font)
            text_width = bbox_text[2] - bbox_text[0]
            max_text_width = max(max_text_width, text_width)
        
        info_box_width = max_text_width + 12
        info_box_height = len(info_lines) * line_height + 8
        
        # ç¡®å®šæ–‡æœ¬æ¡†ä½ç½®ï¼ˆå°½é‡åœ¨æˆ¿é—´åŒºåŸŸå†…éƒ¨ï¼‰
        info_x = max(min_x + 5, center_x - info_box_width // 2)
        info_y = max(min_y + 5, center_y - info_box_height // 2)
        
        # ç¡®ä¿æ–‡æœ¬æ¡†ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
        if info_x + info_box_width > w:
            info_x = w - info_box_width - 5
        if info_y + info_box_height > h:
            info_y = h - info_box_height - 5
        if info_x < 5:
            info_x = 5
        if info_y < 5:
            info_y = 5
        
        # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯æ¡†
        background_color = (0, 0, 0, 200)  # åŠé€æ˜é»‘è‰²
        background_img = Image.new('RGBA', pil_image.size, (0, 0, 0, 0))
        background_draw = ImageDraw.Draw(background_img)
        background_draw.rectangle([info_x, info_y, info_x + info_box_width, info_y + info_box_height], 
                                fill=background_color)
        
        # å°†èƒŒæ™¯åˆå¹¶åˆ°ä¸»å›¾åƒ
        pil_image = pil_image.convert('RGBA')
        pil_image = Image.alpha_composite(pil_image, background_img)
        pil_image = pil_image.convert('RGB')
        draw = ImageDraw.Draw(pil_image)
        
        # ç»˜åˆ¶è¾¹æ¡†
        draw.rectangle([info_x, info_y, info_x + info_box_width, info_y + info_box_height], 
                      outline=color, width=2)
        
        # ç»˜åˆ¶æ–‡æœ¬ä¿¡æ¯
        for idx, line in enumerate(info_lines):
            text_y = info_y + 8 + idx * line_height
            draw.text((info_x + 6, text_y), line, fill=(255, 255, 255), font=small_font)
        
        # åœ¨æˆ¿é—´ä¸­å¿ƒç»˜åˆ¶æˆ¿é—´ç¼–å·
        number_text = str(i+1)
        # ç»˜åˆ¶æ•°å­—è¾¹æ¡†ï¼ˆé»‘è‰²æè¾¹ï¼‰
        for offset_x in [-1, 0, 1]:
            for offset_y in [-1, 0, 1]:
                if offset_x != 0 or offset_y != 0:
                    draw.text((center_x - 8 + offset_x, center_y - 8 + offset_y), number_text, fill=(0, 0, 0), font=font)
        # ç»˜åˆ¶æ•°å­—æœ¬ä½“ï¼ˆç™½è‰²ï¼‰
        draw.text((center_x - 8, center_y - 8), number_text, fill=(255, 255, 255), font=font)
        
        # æ‰“å°æˆ¿é—´ä¿¡æ¯
        print(f"   {i+1}. {emoji} {name}")
        print(f"      ğŸ“ åæ ‡èŒƒå›´: ({min_x}, {min_y}) -> ({max_x}, {max_y})")
        print(f"      ğŸ“ å°ºå¯¸: {width} Ã— {height} åƒç´ ")
        print(f"      ğŸ“ é¢ç§¯: {area} åƒç´ Â²")
        print(f"      ğŸ¯ ä¸­å¿ƒç‚¹: ({center_x}, {center_y})")
        print()
    
    # ä¿å­˜ç»“æœå›¾åƒ
    pil_image.save(output_path)
    print(f"ğŸ“¸ æˆ¿é—´åŒºåŸŸæ ‡æ³¨å›¾åƒå·²ä¿å­˜: {output_path}")
    
    # è½¬æ¢å›numpy arrayè¿”å›
    return np.array(pil_image)

def draw_room_detection_boxes(original_image, ocr_results, output_path):
    """
    åœ¨åŸå§‹å›¾åƒä¸Šç»˜åˆ¶æ‰€æœ‰æˆ¿é—´æ£€æµ‹æ¡†å’Œåæ ‡ä¿¡æ¯
    
    Args:
        original_image: åŸå§‹å›¾åƒ (numpy array)
        ocr_results: OCRæ£€æµ‹ç»“æœåˆ—è¡¨
        output_path: è¾“å‡ºå›¾åƒè·¯å¾„
    """
    import cv2
    from PIL import Image, ImageDraw, ImageFont
    
    # è½¬æ¢ä¸ºPILå›¾åƒä»¥ä¾¿å¤„ç†ä¸­æ–‡
    pil_image = Image.fromarray(original_image)
    draw = ImageDraw.Draw(pil_image)
    
    # å®šä¹‰ä¸åŒæˆ¿é—´ç±»å‹çš„é¢œè‰² (RGBæ ¼å¼ï¼ŒPILä½¿ç”¨RGB)
    room_colors = {
        'bedroom': (255, 255, 0),      # é»„è‰² - å§å®¤
        'bathroom': (0, 0, 255),       # è“è‰² - å«ç”Ÿé—´  
        'living_room': (0, 255, 0),    # ç»¿è‰² - å®¢å…
        'kitchen': (255, 165, 0),      # æ©™è‰² - å¨æˆ¿
        'balcony': (255, 0, 255),      # æ´‹çº¢è‰² - é˜³å°
        'hall': (128, 0, 128),         # ç´«è‰² - ç„å…³/å¤§å…
        'generic': (255, 255, 255)     # ç™½è‰² - å…¶ä»–æˆ¿é—´
    }
    
    # å°è¯•åŠ è½½ä¸­æ–‡å­—ä½“
    try:
        # Windowsç³»ç»Ÿå­—ä½“è·¯å¾„
        font_path = "C:/Windows/Fonts/simhei.ttf"  # é»‘ä½“
        font = ImageFont.truetype(font_path, 16)
        small_font = ImageFont.truetype(font_path, 12)
    except:
        try:
            # å¤‡é€‰å­—ä½“
            font_path = "C:/Windows/Fonts/msyh.ttc"  # å¾®è½¯é›…é»‘
            font = ImageFont.truetype(font_path, 16)
            small_font = ImageFont.truetype(font_path, 12)
        except:
            # ä½¿ç”¨é»˜è®¤å­—ä½“
            font = ImageFont.load_default()
            small_font = ImageFont.load_default()
    
def draw_room_detection_boxes(original_image, ocr_results, output_path):
    """
    åœ¨åŸå§‹å›¾åƒä¸Šç»˜åˆ¶æ‰€æœ‰æˆ¿é—´æ£€æµ‹æ¡†å’Œåæ ‡ä¿¡æ¯
    
    Args:
        original_image: åŸå§‹å›¾åƒ (numpy array)
        ocr_results: OCRæ£€æµ‹ç»“æœåˆ—è¡¨
        output_path: è¾“å‡ºå›¾åƒè·¯å¾„
    """
    import cv2
    from PIL import Image, ImageDraw, ImageFont
    
    # è½¬æ¢ä¸ºPILå›¾åƒä»¥ä¾¿å¤„ç†ä¸­æ–‡
    pil_image = Image.fromarray(original_image)
    draw = ImageDraw.Draw(pil_image)
    
    # å®šä¹‰ä¸åŒæˆ¿é—´ç±»å‹çš„é¢œè‰² (RGBæ ¼å¼ï¼ŒPILä½¿ç”¨RGB)
    room_colors = {
        'bedroom': (255, 255, 0),      # é»„è‰² - å§å®¤
        'bathroom': (0, 0, 255),       # è“è‰² - å«ç”Ÿé—´  
        'living_room': (0, 255, 0),    # ç»¿è‰² - å®¢å…
        'kitchen': (255, 165, 0),      # æ©™è‰² - å¨æˆ¿
        'balcony': (255, 0, 255),      # æ´‹çº¢è‰² - é˜³å°
        'hall': (128, 0, 128),         # ç´«è‰² - ç„å…³/å¤§å…
        'generic': (128, 128, 128)     # ç°è‰² - å…¶ä»–æˆ¿é—´
    }
    
    # å°è¯•åŠ è½½ä¸­æ–‡å­—ä½“
    try:
        # Windowsç³»ç»Ÿå­—ä½“è·¯å¾„
        font_path = "C:/Windows/Fonts/simhei.ttf"  # é»‘ä½“
        font = ImageFont.truetype(font_path, 16)
        small_font = ImageFont.truetype(font_path, 12)
    except:
        try:
            # å¤‡é€‰å­—ä½“
            font_path = "C:/Windows/Fonts/msyh.ttc"  # å¾®è½¯é›…é»‘
            font = ImageFont.truetype(font_path, 16)
            small_font = ImageFont.truetype(font_path, 12)
        except:
            # ä½¿ç”¨é»˜è®¤å­—ä½“
            font = ImageFont.load_default()
            small_font = ImageFont.load_default()

    # æˆ¿é—´ç±»å‹æ£€æµ‹çš„å…³é”®è¯æ˜ å°„
    room_keywords = {
        'bedroom': ['å§å®¤', 'bedroom', 'æˆ¿é—´', 'ç¡æˆ¿'],
        'bathroom': ['å«ç”Ÿé—´', 'æ´—æ‰‹é—´', 'å•æ‰€', 'æµ´å®¤', 'bathroom', 'toilet', 'wc'],
        'living_room': ['å®¢å…', 'é¤å…', 'èµ·å±…å®¤', 'living', 'dining'],
        'kitchen': ['å¨æˆ¿', 'kitchen', 'å¨', 'çƒ¹é¥ª'],
        'balcony': ['é˜³å°', 'balcony', 'éœ²å°'],
        'hall': ['ç„å…³', 'å¤§å…', 'è¿‡é“', 'èµ°å»Š', 'å…¥å£', 'hall', 'entrance']
    }
    
    def get_room_type(text):
        """æ ¹æ®æ–‡å­—å†…å®¹åˆ¤æ–­æˆ¿é—´ç±»å‹"""
        text_lower = text.lower()
        for room_type, keywords in room_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    return room_type
        return 'generic'
    
    print(f"\nğŸ¯ ç»˜åˆ¶æˆ¿é—´æ£€æµ‹æ¡†å’Œåæ ‡ä¿¡æ¯:")
    print(f"ğŸ“¸ å…±æ£€æµ‹åˆ° {len(ocr_results)} ä¸ªæ–‡å­—åŒºåŸŸ")
    
    # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ¯ä¸ªæ£€æµ‹åˆ°çš„æˆ¿é—´
    room_count = {}
    for i, item in enumerate(ocr_results):
        text = item.get('text', '')
        confidence = item.get('confidence', 0)
        bbox = item.get('bbox', (0, 0, 0, 0))
        
        # è·³è¿‡ç½®ä¿¡åº¦è¿‡ä½çš„æ£€æµ‹ç»“æœ
        if confidence < 0.3:
            continue
            
        room_type = get_room_type(text)
        color = room_colors.get(room_type, room_colors['generic'])
        
        # ç»Ÿè®¡æˆ¿é—´æ•°é‡
        if room_type not in room_count:
            room_count[room_type] = 0
        room_count[room_type] += 1
        
        # è·å–è¾¹ç•Œæ¡†åæ ‡
        x, y, w, h = bbox
        x1, y1 = int(x), int(y)
        x2, y2 = int(x + w), int(y + h)
        
        # è®¡ç®—é¢ç§¯ï¼ˆåƒç´ é¢ç§¯ï¼‰
        area_pixels = w * h
        
        # ä½¿ç”¨PILç»˜åˆ¶è¾¹ç•Œæ¡†
        draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
        
        # å‡†å¤‡æ ‡ç­¾æ–‡å­—
        room_emoji = {
            'bedroom': 'ğŸ›ï¸',
            'bathroom': 'ğŸš¿', 
            'living_room': 'ğŸ›‹ï¸',
            'kitchen': 'ğŸ³',
            'balcony': 'ğŸŒ¿',
            'hall': 'ğŸšª',
            'generic': 'ğŸ '
        }
        
        emoji = room_emoji.get(room_type, 'ğŸ ')
        
        # åˆ›å»ºä¿¡æ¯æ–‡æœ¬ï¼ˆä¸ä½¿ç”¨emojiï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜ï¼‰
        info_lines = [
            f"{text}",
            f"åæ ‡: ({x1},{y1})-({x2},{y2})",
            f"å°ºå¯¸: {w:.0f}x{h:.0f}px",
            f"é¢ç§¯: {area_pixels:.0f}pxÂ²",
            f"ç½®ä¿¡åº¦: {confidence:.2f}"
        ]
        
        # è®¡ç®—ä¿¡æ¯æ¡†ä½ç½®
        info_x = x2 + 5
        info_y = y1
        
        # è®¡ç®—æ–‡æœ¬æ¡†å¤§å°
        max_text_width = 0
        line_height = 18
        for line in info_lines:
            bbox_text = draw.textbbox((0, 0), line, font=small_font)
            text_width = bbox_text[2] - bbox_text[0]
            max_text_width = max(max_text_width, text_width)
        
        info_box_width = max_text_width + 10
        info_box_height = len(info_lines) * line_height + 10
        
        # æ£€æŸ¥è¾¹ç•Œï¼Œå¦‚æœè¶…å‡ºå›¾åƒèŒƒå›´åˆ™è°ƒæ•´ä½ç½®
        img_width, img_height = pil_image.size
        if info_x + info_box_width > img_width:
            info_x = x1 - info_box_width - 5
        if info_y + info_box_height > img_height:
            info_y = img_height - info_box_height - 5
        if info_x < 0:
            info_x = 5
        if info_y < 0:
            info_y = 5
        
        # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯æ¡†
        background_color = (0, 0, 0, 180)  # åŠé€æ˜é»‘è‰²
        background_img = Image.new('RGBA', pil_image.size, (0, 0, 0, 0))
        background_draw = ImageDraw.Draw(background_img)
        background_draw.rectangle([info_x, info_y, info_x + info_box_width, info_y + info_box_height], 
                                fill=background_color)
        
        # å°†èƒŒæ™¯åˆå¹¶åˆ°ä¸»å›¾åƒ
        pil_image = pil_image.convert('RGBA')
        pil_image = Image.alpha_composite(pil_image, background_img)
        pil_image = pil_image.convert('RGB')
        draw = ImageDraw.Draw(pil_image)
        
        # ç»˜åˆ¶è¾¹æ¡†
        draw.rectangle([info_x, info_y, info_x + info_box_width, info_y + info_box_height], 
                      outline=color, width=2)
        
        # ç»˜åˆ¶æ–‡æœ¬ä¿¡æ¯
        for idx, line in enumerate(info_lines):
            text_y = info_y + 8 + idx * line_height
            draw.text((info_x + 5, text_y), line, fill=(255, 255, 255), font=small_font)
        
        # åœ¨æ¡†çš„ä¸­å¿ƒç»˜åˆ¶åºå·
        center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2
        draw.text((center_x - 5, center_y - 8), str(i+1), fill=(255, 255, 255), font=font)
        
        # æ‰“å°åæ ‡ä¿¡æ¯
        print(f"   {i+1}. {emoji} {text}")
        print(f"      ğŸ¯ æˆ¿é—´ç±»å‹: {room_type}")
        print(f"      ğŸ“ åæ ‡: ({x1}, {y1}) -> ({x2}, {y2})")
        print(f"      ğŸ“ å°ºå¯¸: {w}Ã—{h} åƒç´ ")
        print(f"      ğŸšï¸ ç½®ä¿¡åº¦: {confidence:.3f}")
        print()
    
    # æ‰“å°æˆ¿é—´ç»Ÿè®¡
    print(f"ğŸ“Š æˆ¿é—´ç±»å‹ç»Ÿè®¡:")
    for room_type, count in room_count.items():
        emoji = room_emoji.get(room_type, 'ğŸ ')
        print(f"   {emoji} {room_type}: {count} ä¸ª")
    
    # ä¿å­˜å›¾åƒ
    pil_image.save(output_path)
    print(f"ğŸ“¸ æˆ¿é—´æ£€æµ‹æ¡†å›¾åƒå·²ä¿å­˜: {output_path}")
    
    # è½¬æ¢å›numpy arrayè¿”å›
    return np.array(pil_image)

def imread(path, mode='RGB'):
    """Read image using PIL"""
    img = Image.open(path)
    if mode == 'RGB':
        img = img.convert('RGB')
    elif mode == 'L':
        img = img.convert('L')
    return np.array(img)

def imresize(img, size):
    """Resize image using PIL"""
    if len(img.shape) == 2:  # Grayscale
        img_pil = Image.fromarray(img, mode='L')
    else:  # RGB
        img_pil = Image.fromarray(img, mode='RGB')
    
    resized = img_pil.resize((size[1], size[0]), Image.LANCZOS)
    return np.array(resized)

# è§£æå‚æ•°
parser = argparse.ArgumentParser()
parser.add_argument('im_path', type=str, nargs='?', default='./demo/demo.jpg',
                    help='input image path')
parser.add_argument('--disable_closet', action='store_true',
                    help='map closet predictions to background')

def simple_connected_components(mask):
    """Simple connected component labeling without scipy"""
    h, w = mask.shape
    labels = np.zeros_like(mask, dtype=int)
    current_label = 0
    
    def flood_fill(start_y, start_x, label):
        """Flood fill algorithm for connected components"""
        stack = [(start_y, start_x)]
        while stack:
            y, x = stack.pop()
            if (y < 0 or y >= h or x < 0 or x >= w or 
                labels[y, x] != 0 or not mask[y, x]):
                continue
            
            labels[y, x] = label
            # Add 4-connected neighbors
            stack.extend([(y+1, x), (y-1, x), (y, x+1), (y, x-1)])
    
    for y in range(h):
        for x in range(w):
            if mask[y, x] and labels[y, x] == 0:
                current_label += 1
                flood_fill(y, x, current_label)
    
    return labels, current_label

def create_precise_room_area(floorplan, center_x, center_y, room_label, img_h, img_w):
    """ç²¾å‡†æˆ¿é—´åŒºåŸŸç”Ÿæˆç®—æ³•ï¼šç»§æ‰¿demo.pyçš„ä¼˜ç§€ç®—æ³•ï¼Œé€‚é…æ‰€æœ‰æˆ¿é—´ç±»å‹"""
    h, w = floorplan.shape
    room_name = {1: "å‚¨ç‰©é—´", 2: "å«ç”Ÿé—´", 3: "å®¢å…", 4: "å§å®¤", 5: "ç„å…³", 6: "é˜³å°", 7: "å¨æˆ¿"}.get(room_label, f"æˆ¿é—´{room_label}")
    
    print(f"      ğŸ  æ™ºèƒ½ç”Ÿæˆ{room_name}åŒºåŸŸ: ä¸­å¿ƒ({center_x}, {center_y})")
    
    # é¦–å…ˆæ£€æŸ¥ä¸­å¿ƒç‚¹æ˜¯å¦åœ¨æœ‰æ•ˆåŒºåŸŸï¼ˆéå¢™å£ï¼‰
    if floorplan[center_y, center_x] in [9, 10]:
        print(f"      âš ï¸ ä¸­å¿ƒç‚¹åœ¨å¢™å£ä¸Šï¼Œå¯»æ‰¾é™„è¿‘çš„æœ‰æ•ˆåŒºåŸŸ")
        # å¯»æ‰¾é™„è¿‘çš„éå¢™å£åŒºåŸŸ
        found_valid = False
        for radius in range(1, 15):  # æ‰©å¤§æœç´¢èŒƒå›´
            for dy in range(-radius, radius+1):
                for dx in range(-radius, radius+1):
                    new_y, new_x = center_y + dy, center_x + dx
                    if (0 <= new_y < h and 0 <= new_x < w and 
                        floorplan[new_y, new_x] not in [9, 10]):
                        center_x, center_y = new_x, new_y
                        found_valid = True
                        break
                if found_valid:
                    break
            if found_valid:
                break
        
        if not found_valid:
            print(f"      âŒ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„{room_name}ä¸­å¿ƒç‚¹")
            return np.zeros((h, w), dtype=bool)
    
    print(f"      âœ… ä½¿ç”¨ä¸­å¿ƒç‚¹: ({center_x}, {center_y})")
    
    # ä½¿ç”¨æ³›æ´ªç®—æ³•æ‰¾åˆ°åŒ…å«ä¸­å¿ƒç‚¹çš„è¿é€šåŒºåŸŸ
    def flood_fill_room(start_x, start_y):
        """æ‰¾åˆ°åŒ…å«èµ·å§‹ç‚¹çš„å®Œæ•´æˆ¿é—´åŒºåŸŸ"""
        visited = np.zeros((h, w), dtype=bool)
        room_mask = np.zeros((h, w), dtype=bool)
        stack = [(start_x, start_y)]
        
        while stack:
            x, y = stack.pop()
            if (x < 0 or x >= w or y < 0 or y >= h or 
                visited[y, x] or floorplan[y, x] in [9, 10]):
                continue
            
            visited[y, x] = True
            room_mask[y, x] = True
            
            # æ·»åŠ 4è¿é€šçš„é‚»å±…
            stack.extend([(x+1, y), (x-1, y), (x, y+1), (x, y-1)])
        
        return room_mask
    
    # è·å–åŒ…å«æˆ¿é—´ä¸­å¿ƒçš„å®Œæ•´æˆ¿é—´
    room_mask = flood_fill_room(center_x, center_y)
    room_pixels = np.sum(room_mask)
    
    # æ ¹æ®æˆ¿é—´ç±»å‹è®¾ç½®æœ€å°é¢ç§¯é˜ˆå€¼
    min_room_sizes = {
        1: 50,    # å‚¨ç‰©é—´ï¼šæœ€å°
        2: 80,    # å«ç”Ÿé—´ï¼šè¾ƒå°
        3: 200,   # å®¢å…ï¼šè¾ƒå¤§
        4: 150,   # å§å®¤ï¼šä¸­ç­‰
        5: 100,   # ç„å…³ï¼šä¸­ç­‰åå°
        6: 80,    # é˜³å°ï¼šè¾ƒå°
        7: 100    # å¨æˆ¿ï¼šä¸­ç­‰åå°
    }
    
    min_size = min_room_sizes.get(room_label, 100)
    
    if room_pixels < min_size:
        print(f"      âŒ æˆ¿é—´å¤ªå°({room_pixels}åƒç´  < {min_size})ï¼Œä¸é€‚åˆåš{room_name}")
        return np.zeros((h, w), dtype=bool)
    
    print(f"      ğŸ“ å‘ç°æˆ¿é—´åŒºåŸŸ: {room_pixels} åƒç´ ")
    
    # è®¡ç®—æˆ¿é—´çš„è¾¹ç•Œæ¡†
    room_coords = np.where(room_mask)
    min_y, max_y = np.min(room_coords[0]), np.max(room_coords[0])
    min_x, max_x = np.min(room_coords[1]), np.max(room_coords[1])
    room_width = max_x - min_x + 1
    room_height = max_y - min_y + 1
    
    print(f"      ğŸ“ æˆ¿é—´è¾¹ç•Œ: ({min_x},{min_y}) åˆ° ({max_x},{max_y}), å°ºå¯¸{room_width}x{room_height}")
    
    # æ ¹æ®æˆ¿é—´ç±»å‹è®¾ç½®å ç”¨æ¯”ä¾‹å’Œç†æƒ³å¤§å°
    room_configs = {
        1: {'max_ratio': 0.8, 'area_ratio': 0.02, 'ideal_shape': 'compact'},     # å‚¨ç‰©é—´ï¼šç´§å‡‘
        2: {'max_ratio': 0.8, 'area_ratio': 0.04, 'ideal_shape': 'compact'},     # å«ç”Ÿé—´ï¼šç´§å‡‘
        3: {'max_ratio': 0.9, 'area_ratio': 0.12, 'ideal_shape': 'rectangular'}, # å®¢å…ï¼šè¾ƒå¤§çŸ©å½¢
        4: {'max_ratio': 0.85, 'area_ratio': 0.08, 'ideal_shape': 'rectangular'}, # å§å®¤ï¼šä¸­ç­‰çŸ©å½¢
        5: {'max_ratio': 0.8, 'area_ratio': 0.06, 'ideal_shape': 'elongated'},   # ç„å…³ï¼šç‹­é•¿
        6: {'max_ratio': 0.8, 'area_ratio': 0.05, 'ideal_shape': 'compact'},     # é˜³å°ï¼šç´§å‡‘
        7: {'max_ratio': 0.8, 'area_ratio': 0.06, 'ideal_shape': 'square'}       # å¨æˆ¿ï¼šæ–¹å½¢
    }
    
    config = room_configs.get(room_label, {'max_ratio': 0.8, 'area_ratio': 0.06, 'ideal_shape': 'square'})
    
    # æ ¹æ®æˆ¿é—´å¤§å°ç¡®å®šæˆ¿é—´å°ºå¯¸ï¼ˆä¸èƒ½è¶…è¿‡æˆ¿é—´çš„è®¾å®šæ¯”ä¾‹ï¼‰
    max_room_width = int(room_width * config['max_ratio'])
    max_room_height = int(room_height * config['max_ratio'])
    
    # è®¡ç®—ç†æƒ³çš„æˆ¿é—´å°ºå¯¸
    total_area = h * w
    target_area = min(total_area * config['area_ratio'], room_pixels * config['max_ratio'])
    target_size = int(np.sqrt(target_area))
    
    # é™åˆ¶æˆ¿é—´å¤§å°
    min_size = 15
    target_size = max(min_size, min(target_size, min(max_room_width, max_room_height)))
    
    print(f"      ğŸ¯ ç›®æ ‡{room_name}å°ºå¯¸: {target_size}x{target_size}")
    
    # åœ¨æˆ¿é—´å†…åˆ›å»ºä»¥ä¸­å¿ƒç‚¹ä¸ºä¸­å¿ƒçš„æˆ¿é—´åŒºåŸŸ
    half_size = target_size // 2
    
    # ç¡®ä¿æˆ¿é—´åŒºåŸŸåœ¨æˆ¿é—´è¾¹ç•Œå†…
    room_left = max(min_x, center_x - half_size)
    room_right = min(max_x + 1, center_x + half_size)
    room_top = max(min_y, center_y - half_size)
    room_bottom = min(max_y + 1, center_y + half_size)
    
    # æ ¹æ®æˆ¿é—´ç±»å‹è°ƒæ•´å½¢çŠ¶
    if config['ideal_shape'] == 'elongated':  # ç„å…³ï¼šç‹­é•¿å½¢
        # ä¼˜å…ˆæ‰©å±•é•¿åº¦æ–¹å‘
        if room_width > room_height:
            room_left = max(min_x, center_x - target_size)
            room_right = min(max_x + 1, center_x + target_size)
        else:
            room_top = max(min_y, center_y - target_size)
            room_bottom = min(max_y + 1, center_y + target_size)
    elif config['ideal_shape'] == 'rectangular':  # å®¢å…ã€å§å®¤ï¼šçŸ©å½¢
        # é€‚å½“æ‰©å±•ä¸ºçŸ©å½¢
        if room_width > room_height:
            extra = target_size // 3
            room_left = max(min_x, center_x - half_size - extra)
            room_right = min(max_x + 1, center_x + half_size + extra)
        else:
            extra = target_size // 3
            room_top = max(min_y, center_y - half_size - extra)
            room_bottom = min(max_y + 1, center_y + half_size + extra)
    # å…¶ä»–ç±»å‹ä¿æŒé»˜è®¤çš„æ­£æ–¹å½¢æˆ–ç´§å‡‘å½¢çŠ¶
    
    # åˆ›å»ºæˆ¿é—´æ©ç ï¼Œåªåœ¨æˆ¿é—´åŒºåŸŸå†…
    final_room_mask = np.zeros((h, w), dtype=bool)
    
    for y in range(room_top, room_bottom):
        for x in range(room_left, room_right):
            if room_mask[y, x]:  # åªåœ¨åŸå§‹æˆ¿é—´åŒºåŸŸå†…
                final_room_mask[y, x] = True
    
    actual_width = room_right - room_left
    actual_height = room_bottom - room_top
    actual_pixels = np.sum(final_room_mask)
    
    print(f"      âœ… {room_name}åŒºåŸŸç”Ÿæˆå®Œæˆ:")
    print(f"         è¾¹ç•Œ: ({room_left},{room_top}) åˆ° ({room_right},{room_bottom})")
    print(f"         å°ºå¯¸: {actual_width}x{actual_height}")
    print(f"         æœ‰æ•ˆåƒç´ : {actual_pixels}")
    
    return final_room_mask

def enhance_room_detection(floorplan, ocr_results):
    """ç²¾å‡†æˆ¿é—´è¯†åˆ«ç³»ç»Ÿï¼šç»§æ‰¿demo.pyçš„ä¼˜ç§€ç®—æ³•ï¼Œç²¾ç¡®è¯†åˆ«æ‰€æœ‰æˆ¿é—´ç±»å‹"""
    from utils.ocr_enhanced import TEXT_LABEL_MAP, ROOM_TYPE_NAMES, ROOM_TYPE_EMOJIS
    
    enhanced = floorplan.copy()
    h, w = enhanced.shape
    
    # ç»Ÿè®¡ä¸åŒç±»å‹çš„OCRæ£€æµ‹ç»“æœ
    ocr_detections = {}
    room_text_found = False
    
    if ocr_results:
        for ocr_item in ocr_results:
            text = ocr_item['text'].strip()
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•æˆ¿é—´ç±»å‹
            for room_text, label in TEXT_LABEL_MAP.items():
                if room_text in text.lower() or room_text == text:
                    if label not in ocr_detections:
                        ocr_detections[label] = []
                    ocr_detections[label].append({
                        'text': text,
                        'original_text': ocr_item['text'],
                        'confidence': ocr_item.get('confidence', 1.0),
                        'bbox': ocr_item['bbox']
                    })
                    room_text_found = True
                    emoji = ROOM_TYPE_EMOJIS.get(label, "ğŸ ")
                    room_name = ROOM_TYPE_NAMES.get(label, f"æˆ¿é—´{label}")
                    print(f"{emoji} OCRæ£€æµ‹åˆ°{room_name}æ–‡å­—: '{text}' (ç½®ä¿¡åº¦: {ocr_item.get('confidence', 1.0):.3f})")
                    break

    print(f"ğŸ¯ OCRæ£€æµ‹åˆ° {len(ocr_detections)} ç§æˆ¿é—´ç±»å‹ï¼Œå…± {sum(len(items) for items in ocr_detections.values())} ä¸ªæˆ¿é—´æ ‡è¯†")
    if room_text_found:
        print("âœ… ä½¿ç”¨OCRä¼˜å…ˆçš„ç²¾å‡†æˆ¿é—´è¯†åˆ«ç­–ç•¥")
    else:
        print("ğŸ“ OCRæœªæ£€æµ‹åˆ°æ˜ç¡®çš„æˆ¿é—´æ–‡å­—ï¼Œä½¿ç”¨ç©ºé—´åˆ†ææ–¹æ³•...")

    # ç²¾å‡†å¤„ç†æ¯ç§æˆ¿é—´ç±»å‹
    for room_label, detections in ocr_detections.items():
        room_name = ROOM_TYPE_NAMES.get(room_label, f"æˆ¿é—´{room_label}")
        emoji = ROOM_TYPE_EMOJIS.get(room_label, "ğŸ ")
        
        print(f"\n{emoji} ç²¾å‡†å¤„ç†{room_name}: {len(detections)} ä¸ªOCRæ£€æµ‹")
        
        for detection in detections:
            # é€‰æ‹©æœ€é«˜ç½®ä¿¡åº¦çš„æ£€æµ‹ç»“æœ
            best_detection = max(detections, key=lambda x: x['confidence']) if len(detections) > 1 else detection
            
            x, y, w, h = best_detection['bbox']
            center_x = x + w // 2
            center_y = y + h // 2
            confidence = best_detection['confidence']
            
            print(f"   ğŸ“ é€‰æ‹©æœ€å¯é çš„{room_name}: '{best_detection['text']}' (ç½®ä¿¡åº¦: {confidence:.3f})")
            print(f"   ğŸ¯ {room_name}ä¸­å¿ƒä½ç½®: ({center_x}, {center_y})")
            
            # ä½¿ç”¨demo.pyçš„ç²¾å‡†ç®—æ³•ç”Ÿæˆæˆ¿é—´åŒºåŸŸ
            room_mask = create_precise_room_area(enhanced, center_x, center_y, room_label, h, w)
            if np.sum(room_mask) > 0:
                enhanced[room_mask] = room_label
                room_pixels = np.sum(room_mask)
                print(f"   âœ… ç”Ÿæˆç²¾å‡†{room_name}åŒºåŸŸ: {room_pixels} åƒç´ ")
            else:
                print(f"   âš ï¸ æ— æ³•ä¸º{room_name}ç”Ÿæˆæœ‰æ•ˆåŒºåŸŸ")
            
            # æ¯ç§æˆ¿é—´ç±»å‹åªå¤„ç†ç½®ä¿¡åº¦æœ€é«˜çš„ä¸€ä¸ªæ£€æµ‹ç»“æœ
            break

    # å¯¹äºæ²¡æœ‰OCRæ£€æµ‹åˆ°çš„æˆ¿é—´ï¼Œä½¿ç”¨ç©ºé—´åˆ†æ
    return enhanced, ocr_detections

def apply_ocr_room_detections(floorplan, ocr_detections):
    """åº”ç”¨OCRæ£€æµ‹åˆ°çš„æ‰€æœ‰æˆ¿é—´ç±»å‹ï¼Œä½¿ç”¨ç²¾å‡†çš„æˆ¿é—´è¯†åˆ«ç®—æ³•"""
    from utils.ocr_enhanced import ROOM_TYPE_NAMES, ROOM_TYPE_EMOJIS
    
    enhanced = floorplan.copy()
    h, w = enhanced.shape
    
    # ä¸ºæ¯ç§æˆ¿é—´ç±»å‹åº”ç”¨ç²¾å‡†çš„OCRæ£€æµ‹ç»“æœ
    for room_label, detections in ocr_detections.items():
        room_name = ROOM_TYPE_NAMES.get(room_label, f"æˆ¿é—´{room_label}")
        emoji = ROOM_TYPE_EMOJIS.get(room_label, "ğŸ ")
        
        print(f"{emoji} ç²¾å‡†å¤„ç†{room_name}è¯†åˆ«: {len(detections)} ä¸ªæ£€æµ‹ç»“æœ")
        
        # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ
        best_detection = max(detections, key=lambda x: x['confidence'])
        bbox = best_detection['bbox']
        confidence = best_detection['confidence']
        text = best_detection['text']
        
        # åº”ç”¨ç²¾å‡†çš„æˆ¿é—´åŒºåŸŸç”Ÿæˆç®—æ³•
        x, y, bbox_w, bbox_h = bbox
        
        # ç¡®ä¿è¾¹ç•Œæ¡†åœ¨å›¾åƒèŒƒå›´å†…
        x = max(0, min(x, w - 1))
        y = max(0, min(y, h - 1))
        x1 = max(0, min(x + bbox_w, w))
        y1 = max(0, min(y + bbox_h, h))
        
        if x1 > x and y1 > y:
            print(f"   ğŸ“ {room_name}æ–‡å­—ä½ç½®: ({x},{y})-({x1},{y1}), ç½®ä¿¡åº¦: {confidence:.3f}")
            
            # ä»¥OCRæ–‡å­—ä¸ºä¸­å¿ƒï¼Œç”Ÿæˆç²¾å‡†çš„æˆ¿é—´åŒºåŸŸ
            center_x, center_y = (x + x1) // 2, (y + y1) // 2
            
            # ä½¿ç”¨ç²¾å‡†æˆ¿é—´åŒºåŸŸç”Ÿæˆç®—æ³•
            room_mask = create_precise_room_area(enhanced, center_x, center_y, room_label, bbox_h, bbox_w)
            
            if np.sum(room_mask) > 0:
                enhanced[room_mask] = room_label
                room_pixels = np.sum(room_mask)
                print(f"   âœ… åŸºäºOCRç²¾å‡†è¯†åˆ«{room_name}: '{text}' (é¢ç§¯: {room_pixels}åƒç´ )")
            else:
                print(f"   âš ï¸ æ— æ³•ä¸º{room_name}ç”Ÿæˆæœ‰æ•ˆåŒºåŸŸ: '{text}'")
    
    return enhanced



def ind2rgb(ind_im, enable_closet=True):
    """Convert indexed image to RGB"""
    # Use the appropriate color map based on closet setting
    if enable_closet:
        color_map = floorplan_fuse_map_figure
    else:
        # Create a modified map without closet
        color_map = floorplan_fuse_map_figure.copy()
        color_map[1] = color_map[3]  # Map closet to living room color
    
    rgb_im = np.zeros((ind_im.shape[0], ind_im.shape[1], 3), dtype=np.uint8)

    for i, rgb in color_map.items():
        rgb_im[(ind_im==i)] = rgb

    return rgb_im

def main(args):
    enable_closet = not args.disable_closet
    set_closet_enabled(enable_closet)

    # Load input
    im = imread(args.im_path, mode='RGB')
    original_im = im.copy()
    
    # Resize image for network inference
    im = imresize(im, (512, 512))
    
    # For OCR, use larger, enhanced image
    from PIL import Image, ImageEnhance
    ocr_img = Image.fromarray(original_im)
    # Enlarge for better OCR
    ocr_img = ocr_img.resize((ocr_img.width * 2, ocr_img.height * 2), Image.LANCZOS)
    # Enhance contrast
    enhancer = ImageEnhance.Contrast(ocr_img)
    ocr_img = enhancer.enhance(2.5)
    # Enhance sharpness
    enhancer = ImageEnhance.Sharpness(ocr_img)
    ocr_img = enhancer.enhance(2.0)
    ocr_im = np.array(ocr_img)
    
    # Extract textual room labels using OCR with enhanced image
    ocr_results = extract_room_text(ocr_im)
    print(f"ğŸ” åŸå§‹OCRæ£€æµ‹åˆ° {len(ocr_results) if ocr_results else 0} ä¸ªæ–‡å­—ç»“æœ")
    
    # Debug: æ˜¾ç¤ºæ‰€æœ‰OCRæ£€æµ‹ç»“æœ
    if ocr_results:
        print("ğŸ“ OCRåŸå§‹æ£€æµ‹ç»“æœ:")
        for i, item in enumerate(ocr_results):
            text = item.get('text', '')
            confidence = item.get('confidence', 0)
            bbox = item.get('bbox', (0,0,0,0))
            print(f"   {i+1}. æ–‡å­—: '{text}' | ç½®ä¿¡åº¦: {confidence:.3f} | ä½ç½®: {bbox}")
    
    # Scale OCR bounding boxes to match segmentation size (512x512)
    if ocr_results:
        scale_x = im.shape[1] / ocr_im.shape[1]
        scale_y = im.shape[0] / ocr_im.shape[0]
        for item in ocr_results:
            x, y, w, h = item['bbox']
            x = int(x * scale_x)
            y = int(y * scale_y)
            w = int(w * scale_x)
            h = int(h * scale_y)
            item['bbox'] = (x, y, w, h)
    
    # Convert to float and normalize for network inference
    im = im.astype(np.float32) / 255.

    # æ£€æµ‹GPUå¯ç”¨æ€§å¹¶é…ç½®TensorFlow
    gpu_available = len(tf.config.experimental.list_physical_devices('GPU')) > 0 if hasattr(tf.config, 'experimental') else False
    if not gpu_available:
        try:
            # TF 1.xçš„GPUæ£€æµ‹æ–¹æ³•
            from tensorflow.python.client import device_lib
            local_devices = device_lib.list_local_devices()
            gpu_available = any(device.device_type == 'GPU' for device in local_devices)
        except:
            gpu_available = False
    
    print(f"ğŸ’» è®¾å¤‡çŠ¶æ€: {'GPUå¯ç”¨' if gpu_available else 'CPUæ¨¡å¼'}")
    
    # Create tensorflow session with optimized configuration
    config = tf.ConfigProto(
            allow_soft_placement=True,
            log_device_placement=False
    )
    # Enable GPU memory growth to avoid allocation issues
    if gpu_available:
        config.gpu_options.allow_growth = True
        print("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿ")
    else:
        # Disable GPU if not available
        print("ğŸ”§ ä½¿ç”¨CPUè®¡ç®—")
        
    with tf.Session(config=config) as sess:
            
            # Initialize
            sess.run(tf.group(tf.global_variables_initializer(),
                                    tf.local_variables_initializer()))

            # Restore pretrained model
            saver = tf.train.import_meta_graph('./pretrained/pretrained_r3d.meta')
            saver.restore(sess, './pretrained/pretrained_r3d')

            # Get default graph
            graph = tf.get_default_graph()

            # Restore inputs & outputs tensor
            x = graph.get_tensor_by_name('inputs:0')
            room_type_logit = graph.get_tensor_by_name('Cast:0')
            room_boundary_logit = graph.get_tensor_by_name('Cast_1:0')

            # Infer results
            [room_type, room_boundary] = sess.run([room_type_logit, room_boundary_logit],
                                                            feed_dict={x:im.reshape(1,512,512,3)})
            room_type, room_boundary = np.squeeze(room_type), np.squeeze(room_boundary)

            # Merge results
            floorplan = room_type.copy()
            floorplan[room_boundary==1] = 9
            floorplan[room_boundary==2] = 10
            
            # Use OCR labels to refine room categories
            floorplan = fuse_ocr_and_segmentation(floorplan, ocr_results)
            
            # ä½¿ç”¨é€šç”¨æˆ¿é—´è¯†åˆ«ç³»ç»Ÿï¼ˆåŒ…å«å¨æˆ¿è¯†åˆ«ï¼‰
            floorplan, ocr_detections = enhance_room_detection(floorplan, ocr_results)
            
            # åº”ç”¨æ‰€æœ‰OCRæ£€æµ‹åˆ°çš„æˆ¿é—´ç±»å‹ï¼ˆç²¾ç¡®è¯†åˆ«ï¼‰
            if ocr_detections:
                print("ğŸ¯ åº”ç”¨OCRç²¾ç¡®æˆ¿é—´è¯†åˆ«ç»“æœ:")
                floorplan = apply_ocr_room_detections(floorplan, ocr_detections)
                
            # If closet is disabled, map closet areas to bedroom (2)
            if not enable_closet:
                floorplan[floorplan==1] = 2  # Map closet to bedroom
                    
            # Convert to RGB
            floorplan_rgb = ind2rgb(floorplan, True)  # Always use full color map

            # ç¡®ä¿outputç›®å½•å­˜åœ¨
            output_dir = "output"
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)

            # Save raw RGB result directly using PIL
            base_name = os.path.basename(args.im_path).split('.')[0]
            output_name = os.path.join(output_dir, base_name + '_raw_result.png')
            result_img = Image.fromarray(floorplan_rgb, mode='RGB')
            result_img.save(output_name)
            print(f"ğŸ“¸ åŸå§‹RGBç»“æœå·²ä¿å­˜: {output_name}")
            
            # ç»˜åˆ¶æˆ¿é—´æ£€æµ‹æ¡†å’Œåæ ‡ä¿¡æ¯ (OCRæ–‡å­—æ£€æµ‹)
            detection_output = os.path.join(output_dir, base_name + '_room_detection.png')
            draw_room_detection_boxes(original_im, ocr_results, detection_output)
            
            # ç»˜åˆ¶æˆ¿é—´åŒºåŸŸå’Œè¯¦ç»†ä¿¡æ¯ï¼ˆæˆ¿é—´åˆ†å‰²ç»“æœï¼‰
            regions_output = os.path.join(output_dir, base_name + '_room_regions.png')
            draw_room_regions_with_info(original_im, floorplan, ocr_results, regions_output)
            
            # Also create matplotlib version for comparison
            plt.figure(figsize=(12, 6))
            plt.subplot(121)
            plt.imshow(original_im)
            plt.title('åŸå§‹å›¾ç‰‡')
            plt.axis('off')
            
            plt.subplot(122)
            plt.imshow(floorplan_rgb)
            plt.title('æˆ·å‹åˆ†æç»“æœ (ç»¿è‰²=å¨æˆ¿)')
            plt.axis('off')
            
            # Save matplotlib result
            matplotlib_output = os.path.join(output_dir, base_name + '_matplotlib_result.png')
            plt.savefig(matplotlib_output, dpi=300, bbox_inches='tight')
            print(f"ğŸ“¸ Matplotlibç»“æœå·²ä¿å­˜: {matplotlib_output}")
            
            plt.show()

if __name__ == '__main__':
    args = parser.parse_args()
    main(args)
