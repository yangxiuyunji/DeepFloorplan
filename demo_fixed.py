#!/usr/bin/env python3
"""
ä¿®æ”¹demo.pyä»¥ç›´æ¥ä¿å­˜RGBå›¾åƒï¼Œé¿å…matplotlibæ’å€¼
"""
import os
import argparse
import numpy as np
import tensorflow.compat.v1 as tf
import matplotlib.pyplot as plt
import matplotlib
from PIL import Image

# é…ç½®TensorFlowæ—¥å¿—çº§åˆ«ï¼Œå®Œå…¨é™éŸ³å†—é•¿è¾“å‡º
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # åªæ˜¾ç¤ºé”™è¯¯
import warnings
warnings.filterwarnings('ignore')

tf.logging.set_verbosity(tf.logging.ERROR)  # å‡å°‘TensorFlowæ—¥å¿—

# Configure Chinese font support for matplotlib
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# Disable TF 2.x behavior for compatibility
tf.disable_v2_behavior()

# OCR utilities - use enhanced version with PaddleOCR support
from utils.ocr_enhanced import extract_room_text, fuse_ocr_and_segmentation, set_closet_enabled
from utils.rgb_ind_convertor import floorplan_fuse_map, floorplan_fuse_map_figure

def imread(path, mode='RGB'):
    """Read image using PIL"""
    img = Image.open(path)
    if mode == 'RGB':
        img = img.convert('RGB')
    elif mode == 'L':
        img = img.convert('L')
    return np.array(img)

def imresize(img, size):
    """Resize image using PIL"""
    if len(img.shape) == 2:  # Grayscale
        img_pil = Image.fromarray(img, mode='L')
    else:  # RGB
        img_pil = Image.fromarray(img, mode='RGB')
    
    resized = img_pil.resize((size[1], size[0]), Image.LANCZOS)
    return np.array(resized)

# è§£æå‚æ•°
parser = argparse.ArgumentParser()
parser.add_argument('im_path', type=str, nargs='?', default='./demo/demo.jpg',
                    help='input image path')
parser.add_argument('--disable_closet', action='store_true',
                    help='map closet predictions to background')

def simple_connected_components(mask):
    """Simple connected component labeling without scipy"""
    h, w = mask.shape
    labels = np.zeros_like(mask, dtype=int)
    current_label = 0
    
    def flood_fill(start_y, start_x, label):
        """Flood fill algorithm for connected components"""
        stack = [(start_y, start_x)]
        while stack:
            y, x = stack.pop()
            if (y < 0 or y >= h or x < 0 or x >= w or 
                labels[y, x] != 0 or not mask[y, x]):
                continue
            
            labels[y, x] = label
            # Add 4-connected neighbors
            stack.extend([(y+1, x), (y-1, x), (y, x+1), (y, x-1)])
    
    for y in range(h):
        for x in range(w):
            if mask[y, x] and labels[y, x] == 0:
                current_label += 1
                flood_fill(y, x, current_label)
    
    return labels, current_label

def enhance_kitchen_detection(floorplan, ocr_results):
    """Enhance kitchen detection using spatial analysis and OCR results."""
    enhanced = floorplan.copy()
    h, w = enhanced.shape
    
    # First, check for OCR-based kitchen detection
    kitchen_found_by_ocr = False
    if ocr_results:
        for ocr_item in ocr_results:
            text = ocr_item['text'].lower()
            if any(keyword in text for keyword in ['å¨æˆ¿', 'kitchen', 'cook', 'çƒ¹é¥ª']):
                kitchen_found_by_ocr = True
                print(f"ğŸ³ OCRæ£€æµ‹åˆ°å¨æˆ¿æ–‡å­—: '{ocr_item['text']}'")
                break
    
    if not kitchen_found_by_ocr:
        print("ğŸ“ OCRæœªæ£€æµ‹åˆ°å¨æˆ¿æ–‡å­—ï¼Œä½¿ç”¨ç©ºé—´åˆ†ææ–¹æ³•...")
    
    try:
        # Find all areas marked as living_room/dining_room/kitchen (class 3)
        living_areas = (enhanced == 3)
        
        if not np.any(living_areas):
            print("âš ï¸ æœªæ‰¾åˆ°å®¢å…/é¤å…/å¨æˆ¿åŒºåŸŸ")
            return enhanced
        
        # Use simple connected components
        labels, num_regions = simple_connected_components(living_areas)
        
        if num_regions == 0:
            print("âš ï¸ è¿é€šåŒºåŸŸåˆ†æå¤±è´¥")
            return enhanced
        
        print(f"ğŸ” å‘ç° {num_regions} ä¸ªå®¢å…/é¤å…/å¨æˆ¿åŒºåŸŸ")
        
        # Analyze each region
        region_stats = []
        for region_id in range(1, num_regions + 1):
            region_mask = (labels == region_id)
            region_coords = np.where(region_mask)
            
            if len(region_coords[0]) == 0:
                continue
                
            # Calculate region properties
            min_y, max_y = region_coords[0].min(), region_coords[0].max()
            min_x, max_x = region_coords[1].min(), region_coords[1].max()
            
            region_height = max_y - min_y + 1
            region_width = max_x - min_x + 1
            region_area = np.sum(region_mask)
            
            # Calculate various metrics
            aspect_ratio = max(region_width, region_height) / min(region_width, region_height)
            density = region_area / (region_width * region_height)
            relative_area = region_area / (h * w)
            
            # Check if any OCR results suggest this is a kitchen
            has_kitchen_text = False
            if ocr_results:
                for ocr_item in ocr_results:
                    if any(keyword in ocr_item['text'].lower() for keyword in ['å¨æˆ¿', 'kitchen', 'cook', 'çƒ¹é¥ª']):
                        ocr_x, ocr_y, ocr_w, ocr_h = ocr_item['bbox']
                        # Check if OCR text is within this region
                        if (min_x <= ocr_x + ocr_w/2 <= max_x and min_y <= ocr_y + ocr_h/2 <= max_y):
                            has_kitchen_text = True
                            break
            
            region_stats.append({
                'id': region_id,
                'mask': region_mask,
                'area': region_area,
                'relative_area': relative_area,
                'aspect_ratio': aspect_ratio,
                'density': density,
                'width': region_width,
                'height': region_height,
                'has_kitchen_text': has_kitchen_text,
                'bbox': (min_x, min_y, max_x, max_y)
            })
        
        # Sort regions by area (smallest first - kitchens are often smaller)
        region_stats.sort(key=lambda x: x['relative_area'])
        
        # Enhanced heuristics for kitchen detection
        kitchen_assigned = False
        for i, stats in enumerate(region_stats):
            print(f"   åŒºåŸŸ{stats['id']}: é¢ç§¯={stats['relative_area']:.3f}, é•¿å®½æ¯”={stats['aspect_ratio']:.2f}, å¯†åº¦={stats['density']:.2f}")
            
            is_kitchen = False
            reasons = []
            
            # Rule 1: Explicit OCR detection
            if stats['has_kitchen_text']:
                is_kitchen = True
                reasons.append("OCRæ£€æµ‹åˆ°å¨æˆ¿æ–‡å­—")
            
            # Rule 2: Small area + reasonable shape (only assign one kitchen)
            elif (not kitchen_assigned and 
                  stats['relative_area'] < 0.15 and  # Smaller than 15% of total area
                  1.0 < stats['aspect_ratio'] < 4.0 and  # Not too elongated
                  stats['density'] > 0.5):  # Good density
                is_kitchen = True
                reasons.append("å°é¢ç§¯+åˆç†å½¢çŠ¶")
            
            # Rule 3: Very small compact area (only assign one kitchen)
            elif (not kitchen_assigned and
                  stats['relative_area'] < 0.08 and  # Very small area
                  stats['aspect_ratio'] < 3.0 and
                  stats['density'] > 0.6):
                is_kitchen = True
                reasons.append("ç´§å‡‘å‹å¨æˆ¿")
            
            # Rule 4: If multiple regions, the smallest reasonable one might be kitchen
            elif (not kitchen_assigned and
                  len(region_stats) > 1 and i == 0 and  # Smallest region
                  stats['relative_area'] < 0.2 and
                  stats['aspect_ratio'] < 5.0 and
                  stats['density'] > 0.4):
                is_kitchen = True
                reasons.append("å¤šåŒºåŸŸä¸­æœ€å°çš„åˆç†åŒºåŸŸ")
            
            if is_kitchen:
                enhanced[stats['mask']] = 7  # Kitchen class
                print(f"   âœ… è¯†åˆ«ä¸ºå¨æˆ¿: {', '.join(reasons)}")
                kitchen_assigned = True
            else:
                print(f"   âŒ ä¿æŒä¸ºå®¢å…/é¤å…")
                
        if not kitchen_assigned:
            print("âš ï¸ æœªèƒ½è‡ªåŠ¨è¯†åˆ«å¨æˆ¿ï¼Œæ‰€æœ‰åŒºåŸŸä¿æŒä¸ºå®¢å…/é¤å…")
                
    except Exception as e:
        print(f"âš ï¸ ç©ºé—´åˆ†æå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    return enhanced

def ind2rgb(ind_im, enable_closet=True):
    """Convert indexed image to RGB"""
    # Use the appropriate color map based on closet setting
    if enable_closet:
        color_map = floorplan_fuse_map_figure
    else:
        # Create a modified map without closet
        color_map = floorplan_fuse_map_figure.copy()
        color_map[1] = color_map[3]  # Map closet to living room color
    
    rgb_im = np.zeros((ind_im.shape[0], ind_im.shape[1], 3), dtype=np.uint8)

    for i, rgb in color_map.items():
        rgb_im[(ind_im==i)] = rgb

    return rgb_im

def main(args):
    enable_closet = not args.disable_closet
    set_closet_enabled(enable_closet)

    # Load input
    im = imread(args.im_path, mode='RGB')
    original_im = im.copy()
    
    # Resize image for network inference
    im = imresize(im, (512, 512))
    
    # For OCR, use larger, enhanced image
    from PIL import Image, ImageEnhance
    ocr_img = Image.fromarray(original_im)
    # Enlarge for better OCR
    ocr_img = ocr_img.resize((ocr_img.width * 2, ocr_img.height * 2), Image.LANCZOS)
    # Enhance contrast
    enhancer = ImageEnhance.Contrast(ocr_img)
    ocr_img = enhancer.enhance(2.5)
    # Enhance sharpness
    enhancer = ImageEnhance.Sharpness(ocr_img)
    ocr_img = enhancer.enhance(2.0)
    ocr_im = np.array(ocr_img)
    
    # Extract textual room labels using OCR with enhanced image
    ocr_results = extract_room_text(ocr_im)
    # Scale OCR bounding boxes to match segmentation size (512x512)
    if ocr_results:
        scale_x = im.shape[1] / ocr_im.shape[1]
        scale_y = im.shape[0] / ocr_im.shape[0]
        for item in ocr_results:
            x, y, w, h = item['bbox']
            x = int(x * scale_x)
            y = int(y * scale_y)
            w = int(w * scale_x)
            h = int(h * scale_y)
            item['bbox'] = (x, y, w, h)
    
    # Convert to float and normalize for network inference
    im = im.astype(np.float32) / 255.

    # æ£€æµ‹GPUå¯ç”¨æ€§å¹¶é…ç½®TensorFlow
    gpu_available = len(tf.config.experimental.list_physical_devices('GPU')) > 0 if hasattr(tf.config, 'experimental') else False
    if not gpu_available:
        try:
            # TF 1.xçš„GPUæ£€æµ‹æ–¹æ³•
            from tensorflow.python.client import device_lib
            local_devices = device_lib.list_local_devices()
            gpu_available = any(device.device_type == 'GPU' for device in local_devices)
        except:
            gpu_available = False
    
    print(f"ğŸ’» è®¾å¤‡çŠ¶æ€: {'GPUå¯ç”¨' if gpu_available else 'CPUæ¨¡å¼'}")
    
    # Create tensorflow session with optimized configuration
    config = tf.ConfigProto(
            allow_soft_placement=True,
            log_device_placement=False
    )
    # Enable GPU memory growth to avoid allocation issues
    if gpu_available:
        config.gpu_options.allow_growth = True
        print("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿ")
    else:
        # Disable GPU if not available
        print("ğŸ”§ ä½¿ç”¨CPUè®¡ç®—")
        
    with tf.Session(config=config) as sess:
            
            # Initialize
            sess.run(tf.group(tf.global_variables_initializer(),
                                    tf.local_variables_initializer()))

            # Restore pretrained model
            saver = tf.train.import_meta_graph('./pretrained/pretrained_r3d.meta')
            saver.restore(sess, './pretrained/pretrained_r3d')

            # Get default graph
            graph = tf.get_default_graph()

            # Restore inputs & outputs tensor
            x = graph.get_tensor_by_name('inputs:0')
            room_type_logit = graph.get_tensor_by_name('Cast:0')
            room_boundary_logit = graph.get_tensor_by_name('Cast_1:0')

            # Infer results
            [room_type, room_boundary] = sess.run([room_type_logit, room_boundary_logit],
                                                            feed_dict={x:im.reshape(1,512,512,3)})
            room_type, room_boundary = np.squeeze(room_type), np.squeeze(room_boundary)

            # Merge results
            floorplan = room_type.copy()
            floorplan[room_boundary==1] = 9
            floorplan[room_boundary==2] = 10
            
            # Use OCR labels to refine room categories
            floorplan = fuse_ocr_and_segmentation(floorplan, ocr_results)
            
            # Enhance kitchen detection
            floorplan = enhance_kitchen_detection(floorplan, ocr_results)
            
            # If closet is disabled, map closet areas to bedroom (2)
            if not enable_closet:
                floorplan[floorplan==1] = 2  # Map closet to bedroom
                    
            # Convert to RGB
            floorplan_rgb = ind2rgb(floorplan, True)  # Always use full color map

            # Save raw RGB result directly using PIL
            output_name = os.path.basename(args.im_path).split('.')[0] + '_raw_result.png'
            result_img = Image.fromarray(floorplan_rgb, mode='RGB')
            result_img.save(output_name)
            print(f"ğŸ“¸ åŸå§‹RGBç»“æœå·²ä¿å­˜: {output_name}")
            
            # Also create matplotlib version for comparison
            plt.figure(figsize=(12, 6))
            plt.subplot(121)
            plt.imshow(original_im)
            plt.title('åŸå§‹å›¾ç‰‡')
            plt.axis('off')
            
            plt.subplot(122)
            plt.imshow(floorplan_rgb)
            plt.title('æˆ·å‹åˆ†æç»“æœ (ç»¿è‰²=å¨æˆ¿)')
            plt.axis('off')
            
            # Save matplotlib result
            matplotlib_output = os.path.basename(args.im_path).split('.')[0] + '_matplotlib_result.png'
            plt.savefig(matplotlib_output, dpi=300, bbox_inches='tight')
            print(f"ğŸ“¸ Matplotlibç»“æœå·²ä¿å­˜: {matplotlib_output}")
            
            plt.show()

if __name__ == '__main__':
    args = parser.parse_args()
    main(args)
