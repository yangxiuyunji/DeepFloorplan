"""Enhanced OCR utilities with PaddleOCR support for floorplan processing."""

from typing import List, Dict, Tuple, Any
import numpy as np

# Try PaddleOCR first
try:
    from paddleocr import PaddleOCR
    import cv2
    _HAS_PADDLE_OCR = True
    _paddle_ocr_instance = None
    print("ðŸš€ PaddleOCRå¯ç”¨ï¼Œå°†ä¼˜å…ˆä½¿ç”¨")
except Exception:
    _HAS_PADDLE_OCR = False
    print("âš ï¸ PaddleOCRä¸å¯ç”¨ï¼Œä½¿ç”¨Tesseract")

# Fallback to Tesseract
try:
    import pytesseract
    from pytesseract import Output
    from PIL import Image, ImageEnhance
    _HAS_TESSERACT = True
except Exception:
    pytesseract = None
    Output = None
    _HAS_TESSERACT = False

# Text to label mapping (extended for both OCR engines)
TEXT_LABEL_MAP = {
    # bedroom
    'å§å®¤': 4, 'ä¸»å§': 4, 'æ¬¡å§': 4, 'bedroom': 4, 'br': 4,
    # living / dining room
    'å®¢åŽ…': 3, 'living': 3, 'livingroom': 3, 'living room': 3,
    'é¤åŽ…': 3, 'dining': 3, 'diningroom': 3, 'dining room': 3,
    # kitchen - now has its own category
    'åŽ¨æˆ¿': 7, 'kitchen': 7, 'cook': 7, 'çƒ¹é¥ª': 7,
    # bathroom / washroom
    'å«ç”Ÿé—´': 2, 'æ´—æ‰‹é—´': 2, 'æµ´å®¤': 2, 'bathroom': 2,
    'washroom': 2, 'toilet': 2,
    # balcony
    'é˜³å°': 6, 'balcony': 6,
    # hall / lobby
    'çŽ„å…³': 5, 'å¤§åŽ…': 5, 'hall': 5, 'lobby': 5,
    # closet / storage
    'è¡£æŸœ': 1, 'closet': 1
}

# Global closet control
ENABLE_CLOSET = True

def set_closet_enabled(enable: bool) -> None:
    """Globally enable or disable the closet category."""
    global ENABLE_CLOSET
    ENABLE_CLOSET = enable

def extract_room_text(image: Any) -> List[Dict]:
    """Extract room text using the best available OCR engine.
    
    Parameters
    ----------
    image: Any
        Input image (numpy array, PIL Image, or file path)
        
    Returns
    -------
    List[Dict]
        List of detected text with bounding boxes and confidence
    """
    if _HAS_PADDLE_OCR:
        return extract_room_text_paddle(image)
    elif _HAS_TESSERACT:
        return extract_room_text_tesseract(image)
    else:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„OCRå¼•æ“Ž")
        return []

def extract_room_text_paddle(image: Any) -> List[Dict]:
    """Extract room text using PaddleOCR"""
    global _paddle_ocr_instance
    
    if _paddle_ocr_instance is None:
        print("ðŸš€ åˆå§‹åŒ–PaddleOCR...")
        try:
            # è®¾ç½®çŽ¯å¢ƒå˜é‡æ¥å‡å°‘è­¦å‘Šä¿¡æ¯
            import os
            os.environ['PYTHONIOENCODING'] = 'utf-8'
            
            _paddle_ocr_instance = PaddleOCR(lang='ch')
            print("âœ… PaddleOCRåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ PaddleOCRåˆå§‹åŒ–å¤±è´¥: {e}")
            return []
    
    # Process image
    if isinstance(image, str):
        img = cv2.imread(image)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    elif isinstance(image, np.ndarray):
        img = image.copy()
    else:
        img = np.array(image)
    
    # Enhance image for better OCR and keep scale factor for coordinate correction
    enhanced_img, scale_factor = enhance_image_for_paddle_ocr(img)
    
    try:
        results = _paddle_ocr_instance.ocr(enhanced_img)
        extracted_texts = []
        
        # å¤„ç†æ–°çš„PaddleOCRç»“æžœæ ¼å¼
        if results and len(results) > 0:
            result = results[0]  # èŽ·å–ç¬¬ä¸€ä¸ªç»“æžœ
            
            # å°è¯•ç›´æŽ¥è®¿é—®ç»“æžœæ•°æ®
            if 'rec_texts' in result:
                texts = result['rec_texts']
                scores = result['rec_scores']  
                polys = result['rec_polys']
                
                print(f"ðŸ” PaddleOCRæ£€æµ‹åˆ°æ–‡æœ¬: {texts}")
                
                for i, (text, score, poly) in enumerate(zip(texts, scores, polys)):
                    if score > 0.3 and text.strip():
                        # ä»Žè¾¹ç•Œæ¡†å¤šè¾¹å½¢è®¡ç®—çŸ©å½¢è¾¹ç•Œ
                        x_coords = [point[0] for point in poly]
                        y_coords = [point[1] for point in poly]
                        
                        # åæ ‡åŸºäºŽå¢žå¼ºåŽçš„å›¾åƒï¼Œéœ€è¦ç¼©æ”¾å›žåŽŸå§‹å°ºå¯¸
                        x = int(min(x_coords) / scale_factor)
                        y = int(min(y_coords) / scale_factor)
                        w = int((max(x_coords) - min(x_coords)) / scale_factor)
                        h = int((max(y_coords) - min(y_coords)) / scale_factor)
                        
                        extracted_texts.append({
                            'text': text,
                            'bbox': (x, y, w, h),
                            'confidence': score
                        })
                        print(f"ðŸ” PaddleOCR: '{text}' (ç½®ä¿¡åº¦: {score:.3f})")
            
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆå¦‚æžœæœ‰çš„è¯ï¼‰
            elif isinstance(result, list):
                for line in result:
                    if len(line) >= 2:
                        bbox_points = line[0]
                        text_info = line[1]
                        
                        if isinstance(text_info, (list, tuple)) and len(text_info) >= 2:
                            text = text_info[0]
                            confidence = text_info[1]
                        else:
                            text = str(text_info)
                            confidence = 1.0
                        
                        # Convert bbox format
                        x_coords = [point[0] for point in bbox_points]
                        y_coords = [point[1] for point in bbox_points]
                        
                        # å°†åæ ‡ä»Žå¢žå¼ºå›¾åƒç¼©æ”¾å›žåŽŸå§‹å›¾åƒ
                        x = int(min(x_coords) / scale_factor)
                        y = int(min(y_coords) / scale_factor)
                        w = int((max(x_coords) - min(x_coords)) / scale_factor)
                        h = int((max(y_coords) - min(y_coords)) / scale_factor)
                        
                        if confidence > 0.3 and text.strip():
                            extracted_texts.append({
                                'text': text,
                                'bbox': (x, y, w, h),
                                'confidence': confidence
                            })
                            print(f"ðŸ” PaddleOCR: '{text}' (ç½®ä¿¡åº¦: {confidence:.3f})")
        
        print(f"ðŸ“Š PaddleOCRæ£€æµ‹åˆ° {len(extracted_texts)} ä¸ªæ–‡å­—åŒºåŸŸ")
        return extracted_texts
        
    except Exception as e:
        print(f"âŒ PaddleOCRè¯†åˆ«å‡ºé”™: {e}")
        print("ðŸ“‹ è°ƒè¯•ä¿¡æ¯:")
        try:
            debug_results = _paddle_ocr_instance.ocr(enhanced_img)
            print(f"  åŽŸå§‹ç»“æžœç±»åž‹: {type(debug_results)}")
            if debug_results:
                print(f"  ç»“æžœæ•°é‡: {len(debug_results)}")
                print(f"  å®Œæ•´ç»“æžœç»“æž„: {debug_results}")
                
                # å®‰å…¨åœ°æ£€æŸ¥ç¬¬ä¸€ç»„ç»“æžœ
                if len(debug_results) > 0 and debug_results[0] is not None:
                    first_group = debug_results[0]
                    print(f"  ç¬¬ä¸€ç»„ç»“æžœç±»åž‹: {type(first_group)}")
                    print(f"  ç¬¬ä¸€ç»„ç»“æžœæ•°é‡: {len(first_group) if hasattr(first_group, '__len__') else 'N/A'}")
                    
                    if hasattr(first_group, '__len__') and len(first_group) > 0:
                        first_detection = first_group[0]
                        print(f"  ç¬¬ä¸€ä¸ªæ£€æµ‹ç»“æžœ: {first_detection}")
                else:
                    print("  ç¬¬ä¸€ç»„ç»“æžœä¸ºç©ºæˆ–None")
            else:
                print("  ç»“æžœä¸ºNoneæˆ–ç©º")
        except Exception as debug_e:
            print(f"  è°ƒè¯•å¤±è´¥: {debug_e}")
            import traceback
            print(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return []

def enhance_image_for_paddle_ocr(image):
    """Optimize image for PaddleOCR.

    Returns
    -------
    Tuple[np.ndarray, float]
        Enhanced image and the scale factor applied.
    """
    height, width = image.shape[:2]
    scale_factor = max(2.0, 1000.0 / max(height, width))
    new_width = int(width * scale_factor)
    new_height = int(height * scale_factor)
    
    enhanced = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
    
    if len(enhanced.shape) == 3:
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
    
    # Enhance contrast
    enhanced = cv2.convertScaleAbs(enhanced, alpha=1.5, beta=20)
    
    # Denoise
    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)
    
    # Sharpen
    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    enhanced = cv2.filter2D(enhanced, -1, kernel)
    
    # Convert back to RGB
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2RGB)

    return enhanced, scale_factor

def extract_room_text_tesseract(image: Any) -> List[Dict]:
    """Extract room text using Tesseract (fallback)"""
    if not _HAS_TESSERACT:
        return []
    
    try:
        # Convert image to PIL format
        if isinstance(image, str):
            pil_image = Image.open(image)
        elif isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image
        
        # Enhance image
        pil_image = pil_image.convert('RGB')
        enhanced = enhance_image_for_tesseract(pil_image)
        
        # OCR
        custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzåŽ¨æˆ¿å®¢åŽ…å§å®¤å«ç”Ÿé—´é˜³å°'
        data = pytesseract.image_to_data(enhanced, lang='chi_sim+eng', config=custom_config, output_type=Output.DICT)
        
        extracted_texts = []
        n_boxes = len(data['level'])
        
        for i in range(n_boxes):
            text = data['text'][i].strip()
            conf = int(data['conf'][i])
            
            if text and conf > 30:
                x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
                extracted_texts.append({
                    'text': text,
                    'bbox': (x, y, w, h),
                    'confidence': conf / 100.0
                })
                print(f"ðŸ” Tesseract: '{text}' (ç½®ä¿¡åº¦: {conf})")
        
        print(f"ðŸ“Š Tesseractæ£€æµ‹åˆ° {len(extracted_texts)} ä¸ªæ–‡å­—åŒºåŸŸ")
        return extracted_texts
        
    except Exception as e:
        print(f"âŒ Tesseractè¯†åˆ«å‡ºé”™: {e}")
        return []

def enhance_image_for_tesseract(pil_image):
    """Optimize image for Tesseract"""
    # Resize
    width, height = pil_image.size
    scale_factor = max(2.0, 800.0 / max(width, height))
    new_size = (int(width * scale_factor), int(height * scale_factor))
    enhanced = pil_image.resize(new_size, Image.LANCZOS)
    
    # Enhance contrast
    enhancer = ImageEnhance.Contrast(enhanced)
    enhanced = enhancer.enhance(2.5)
    
    # Enhance sharpness
    enhancer = ImageEnhance.Sharpness(enhanced)
    enhanced = enhancer.enhance(2.0)
    
    return enhanced

def text_to_label(text: str) -> int:
    """Convert recognized text to room label.
    
    Returns -1 if text doesn't correspond to any known room type.
    When ENABLE_CLOSET is False, closet-related text maps to background (0).
    """
    # Clean text
    cleaned_text = ''.join(c for c in text if c.isalnum() or c in 'åŽ¨æˆ¿å®¢åŽ…å§å®¤å«ç”Ÿé—´é˜³å°')
    
    label = TEXT_LABEL_MAP.get(cleaned_text.lower(), -1)
    
    # Fuzzy matching
    if label == -1:
        for key, value in TEXT_LABEL_MAP.items():
            if key in cleaned_text or cleaned_text in key:
                label = value
                break
    
    # Handle closet disable
    if label == 1 and not ENABLE_CLOSET:
        return 0
        
    return label

def fuse_ocr_and_segmentation(seg, ocr_results):
    """Fuse OCR results with segmentation map.
    
    Parameters
    ----------
    seg: np.ndarray
        2-D segmentation labels array
    ocr_results: List[Dict]
        OCR results from extract_room_text
        
    Returns
    -------
    np.ndarray
        Fused segmentation result
    """
    fused = seg.copy()
    
    print(f"ðŸ”— èžåˆ {len(ocr_results)} ä¸ªOCRç»“æžœ")
    
    for ocr_item in ocr_results:
        text = ocr_item['text']
        confidence = ocr_item.get('confidence', 1.0)
        x, y, w, h = ocr_item['bbox']
        
        label = text_to_label(text)
        
        if label != -1:
            print(f"ðŸ“ åº”ç”¨OCRæ ‡ç­¾: '{text}' -> æ ‡ç­¾{label} (ç½®ä¿¡åº¦: {confidence:.3f})")
            
            # Ensure bbox is within image bounds
            x = max(0, min(x, seg.shape[1] - 1))
            y = max(0, min(y, seg.shape[0] - 1))
            x1 = max(0, min(x + w, seg.shape[1]))
            y1 = max(0, min(y + h, seg.shape[0]))
            
            if x1 > x and y1 > y:
                region = fused[y:y1, x:x1]
                if region.size > 0:
                    # Preserve boundaries (doors/windows and walls)
                    mask = np.isin(region, [9, 10])
                    region[~mask] = label
                    fused[y:y1, x:x1] = region
    
    # Handle closet disable globally
    if not ENABLE_CLOSET:
        fused[fused == 1] = 0
        
    return fused

# Export main functions
__all__ = ['extract_room_text', 'fuse_ocr_and_segmentation', 'text_to_label',
           'TEXT_LABEL_MAP', 'set_closet_enabled', 'ENABLE_CLOSET']
