"""Enhanced OCR utilities with PaddleOCR support for floorplan processing."""

from typing import List, Dict, Tuple, Any
import numpy as np

# Try PaddleOCR first
try:
    from paddleocr import PaddleOCR
    import cv2
    _HAS_PADDLE_OCR = True
    _paddle_ocr_instance = None
    print("ðŸš€ PaddleOCRå¯ç”¨ï¼Œå°†ä¼˜å…ˆä½¿ç”¨")
except Exception as e:
    _HAS_PADDLE_OCR = False
    print(f"âŒ PaddleOCRå¯¼å…¥å¤±è´¥: {e}")
    print("âš ï¸ æœ¬ç¨‹åºéœ€è¦PaddleOCRæ‰èƒ½æ­£å¸¸å·¥ä½œï¼Œè¯·å®‰è£…PaddleOCR")

# Fallback to Tesseract
try:
    import pytesseract
    from pytesseract import Output
    from PIL import Image, ImageEnhance
    _HAS_TESSERACT = True
except Exception:
    pytesseract = None
    Output = None
    _HAS_TESSERACT = False

# Text to label mapping (enhanced for all room types)
TEXT_LABEL_MAP = {
    # bedroom - å§å®¤ç±» (æ ‡ç­¾4)
    'å§å®¤': 4, 'ä¸»å§': 4, 'æ¬¡å§': 4, 'å§å®¤A': 4, 'å§å®¤B': 4, 'å§å®¤C': 4, 'å§å®¤1': 4, 'å§å®¤2': 4, 'å§å®¤3': 4,
    'bedroom': 4, 'br': 4, 'bed': 4, 'master': 4, 'guest': 4,
    
    # bathroom - å«ç”Ÿé—´ç±» (æ ‡ç­¾2)  
    'å«ç”Ÿé—´': 2, 'æ´—æ‰‹é—´': 2, 'æµ´å®¤': 2, 'å«A': 2, 'å«B': 2, 'å«1': 2, 'å«2': 2,
    'bathroom': 2, 'washroom': 2, 'toilet': 2, 'wc': 2, 'bath': 2,
    
    # living/dining - å®¢åŽ…é¤åŽ…ç±» (æ ‡ç­¾3)
    'å®¢åŽ…': 3, 'é¤åŽ…': 3, 'èµ·å±…å®¤': 3, 'é¥­åŽ…': 3, 'ä¼šå®¢åŽ…': 3,
    'living': 3, 'livingroom': 3, 'living room': 3, 'dining': 3, 'diningroom': 3, 'dining room': 3,
    
    # balcony - é˜³å°ç±» (æ ‡ç­¾6)
    'é˜³å°': 6, 'éœ²å°': 6, 'èŠ±å›­': 6, 'åº­é™¢': 6, 'å¹³å°': 6,
    'balcony': 6, 'terrace': 6, 'patio': 6, 'garden': 6, 'deck': 6,
    
    # kitchen - åŽ¨æˆ¿ç±» (æ ‡ç­¾7)
    'åŽ¨æˆ¿': 7, 'åŽ¨': 7, 'çƒ¹é¥ªé—´': 7, 'æ–™ç†å°': 7,
    'kitchen': 7, 'cook': 7, 'çƒ¹é¥ª': 7, 'kitchenette': 7,
    
    # hall/entrance - çŽ„å…³å¤§åŽ…ç±» (æ ‡ç­¾5)
    'çŽ„å…³': 5, 'å¤§åŽ…': 5, 'é—¨åŽ…': 5, 'è¿‡é“': 5, 'èµ°å»Š': 5, 'å…¥å£': 5,
    'hall': 5, 'lobby': 5, 'entrance': 5, 'corridor': 5, 'foyer': 5,
    
    # closet/storage - å‚¨ç‰©é—´ç±» (æ ‡ç­¾1)
    'è¡£æŸœ': 1, 'å‚¨ç‰©é—´': 1, 'æ‚ç‰©é—´': 1, 'å‚¨è—å®¤': 1, 'è¡£å¸½é—´': 1,
    'closet': 1, 'storage': 1, 'wardrobe': 1, 'pantry': 1
}

# Global closet control - disabled by default
ENABLE_CLOSET = False

# Room type descriptions for logging
ROOM_TYPE_NAMES = {
    1: "å‚¨ç‰©é—´",
    2: "å«ç”Ÿé—´", 
    3: "å®¢åŽ…/é¤åŽ…",
    4: "å§å®¤",
    5: "çŽ„å…³/å¤§åŽ…",
    6: "é˜³å°",
    7: "åŽ¨æˆ¿"
}

# Room type emojis for better visualization
ROOM_TYPE_EMOJIS = {
    1: "ðŸ—„ï¸",
    2: "ðŸš¿", 
    3: "ðŸ›‹ï¸",
    4: "ðŸ›ï¸",
    5: "ðŸšª",
    6: "ðŸŒ¿",
    7: "ðŸ³"
}

def set_closet_enabled(enable: bool) -> None:
    """Globally enable or disable the closet category."""
    global ENABLE_CLOSET
    ENABLE_CLOSET = enable

def reset_paddle_ocr() -> None:
    """Reset PaddleOCR instance to apply new parameters"""
    global _paddle_ocr_instance
    _paddle_ocr_instance = None
    print("ðŸ”„ é‡ç½®PaddleOCRå®žä¾‹")

def extract_room_text(image: Any) -> List[Dict]:
    """Extract room text using PaddleOCR only.
    
    Parameters
    ----------
    image: Any
        Input image (numpy array, PIL Image, or file path)
        
    Returns
    -------
    List[Dict]
        List of detected text with bounding boxes and confidence
    """
    if not _HAS_PADDLE_OCR:
        raise RuntimeError(
            "âŒ PaddleOCRä¸å¯ç”¨ï¼è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…PaddleOCRï¼š\n"
            "   pip install paddlepaddle-gpu==2.5.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
            "   pip install paddleocr==2.6.1.3\n"
            "   æˆ–è€…ä½¿ç”¨CPUç‰ˆæœ¬ï¼špip install paddlepaddle==2.5.2"
        )
    
    print("ðŸŽ¯ æ­£åœ¨ä½¿ç”¨ PaddleOCR è¿›è¡Œä¸­æ–‡æ–‡å­—è¯†åˆ«...")
    return extract_room_text_paddle(image)

def extract_room_text_paddle(image: Any) -> List[Dict]:
    """Extract room text using PaddleOCR with enhanced parameters"""
    global _paddle_ocr_instance
    
    # Check if PaddleOCR is available
    if not _HAS_PADDLE_OCR:
        raise RuntimeError(
            "âŒ PaddleOCRä¸å¯ç”¨ï¼è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…PaddleOCRï¼š\n"
            "   pip install paddlepaddle-gpu==2.5.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
            "   pip install paddleocr==2.6.1.3\n"
            "   æˆ–è€…ä½¿ç”¨CPUç‰ˆæœ¬ï¼špip install paddlepaddle==2.5.2"
        )
    
    if _paddle_ocr_instance is None:
        print("ðŸš€ åˆå§‹åŒ–PaddleOCRï¼ˆå¢žå¼ºæ¨¡å¼ï¼‰...")
        try:
            # è®¾ç½®çŽ¯å¢ƒå˜é‡æ¥å‡å°‘è­¦å‘Šä¿¡æ¯
            import os
            os.environ['PYTHONIOENCODING'] = 'utf-8'
            
            # åŸºäºŽPaddleOCRå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å‚æ•°é…ç½®
            _paddle_ocr_instance = PaddleOCR(
                lang='ch',
                det_db_thresh=0.2,        # åƒç´ åˆ†ç±»é˜ˆå€¼ï¼Œè¶Šå°æ£€æµ‹è¶Šå¤šæ–‡æœ¬ï¼ˆå®˜æ–¹é»˜è®¤0.3ï¼‰
                det_db_box_thresh=0.4,    # æ–‡æœ¬æ¡†ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå‡å°‘æ¼æ£€ï¼ˆå®˜æ–¹é»˜è®¤0.6ï¼‰
                det_db_unclip_ratio=2.5,  # æ–‡æœ¬æ¡†æ‰©å¼ ç³»æ•°ï¼Œå®˜æ–¹æŽ¨è2.5é¿å…è¾¹ç¼˜ä¸¢å¤±
                drop_score=0.3,           # è¯†åˆ«ç»“æžœç½®ä¿¡åº¦è¿‡æ»¤ï¼ˆé»˜è®¤0.5ï¼‰
                use_angle_cls=True,       # å¯ç”¨è§’åº¦åˆ†ç±»å™¨å¤„ç†æ—‹è½¬æ–‡å­—
                cls_thresh=0.8,           # è§’åº¦åˆ†ç±»é˜ˆå€¼ï¼ˆé»˜è®¤0.9ï¼‰
                use_dilation=True,        # å®˜æ–¹æŽ¨è: è†¨èƒ€å¤„ç†æé«˜æ£€æµ‹æ•ˆæžœ
                det_db_score_mode='slow'  # å®˜æ–¹æŽ¨è: æ›´ç²¾ç¡®çš„å¾—åˆ†è®¡ç®—æ¨¡å¼
            )
            print("âœ… PaddleOCRä¸“ä¸šä¼˜åŒ–æ¨¡å¼åˆå§‹åŒ–å®Œæˆ")
            print("   ðŸ“‹ å®˜æ–¹æœ€ä½³å®žè·µå‚æ•°:")
            print("   ðŸ”¹ det_db_thresh=0.2 (æ›´æ•æ„Ÿçš„åƒç´ æ£€æµ‹)")
            print("   ðŸ”¹ det_db_box_thresh=0.4 (å‡å°‘æ¼æ£€)")
            print("   ðŸ”¹ det_db_unclip_ratio=2.5 (å®˜æ–¹æŽ¨èæ‰©å¼ ç³»æ•°)")
            print("   ðŸ”¹ use_dilation=True (è†¨èƒ€å¤„ç†æå‡æ•ˆæžœ)")
            print("   ðŸ”¹ det_db_score_mode='slow' (ç²¾ç¡®å¾—åˆ†è®¡ç®—)")
        except Exception as e:
            print(f"âŒ PaddleOCRåˆå§‹åŒ–å¤±è´¥: {e}")
            return []
    
    # Process image
    if isinstance(image, str):
        img = cv2.imread(image)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    elif isinstance(image, np.ndarray):
        img = image.copy()
    else:
        img = np.array(image)
    
    # Enhance image for better OCR and keep scale factor for coordinate correction
    enhanced_img, scale_factor = enhance_image_for_paddle_ocr(img)
    
    try:
        results = _paddle_ocr_instance.ocr(enhanced_img)
        extracted_texts = []
        
        # å¤„ç†æ–°çš„PaddleOCRç»“æžœæ ¼å¼
        if results and len(results) > 0:
            result = results[0]  # èŽ·å–ç¬¬ä¸€ä¸ªç»“æžœ
            
            # å°è¯•ç›´æŽ¥è®¿é—®ç»“æžœæ•°æ®
            if 'rec_texts' in result:
                texts = result['rec_texts']
                scores = result['rec_scores']  
                polys = result['rec_polys']
                
                print(f"ðŸ” PaddleOCRæ£€æµ‹åˆ°æ–‡æœ¬: {texts}")
                
                for i, (text, score, poly) in enumerate(zip(texts, scores, polys)):
                    if score > 0.3 and text.strip():
                        # ä»Žè¾¹ç•Œæ¡†å¤šè¾¹å½¢è®¡ç®—çŸ©å½¢è¾¹ç•Œ
                        x_coords = [point[0] for point in poly]
                        y_coords = [point[1] for point in poly]
                        
                        # åæ ‡åŸºäºŽå¢žå¼ºåŽçš„å›¾åƒï¼Œéœ€è¦ç¼©æ”¾å›žåŽŸå§‹å°ºå¯¸
                        x = int(min(x_coords) / scale_factor)
                        y = int(min(y_coords) / scale_factor)
                        w = int((max(x_coords) - min(x_coords)) / scale_factor)
                        h = int((max(y_coords) - min(y_coords)) / scale_factor)
                        
                        extracted_texts.append({
                            'text': text,
                            'bbox': (x, y, w, h),
                            'confidence': score
                        })
                        print(f"ðŸ” PaddleOCR: '{text}' (ç½®ä¿¡åº¦: {score:.3f})")
            
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆå¦‚æžœæœ‰çš„è¯ï¼‰
            elif isinstance(result, list):
                for line in result:
                    if len(line) >= 2:
                        bbox_points = line[0]
                        text_info = line[1]
                        
                        if isinstance(text_info, (list, tuple)) and len(text_info) >= 2:
                            text = text_info[0]
                            confidence = text_info[1]
                        else:
                            text = str(text_info)
                            confidence = 1.0
                        
                        # Convert bbox format
                        x_coords = [point[0] for point in bbox_points]
                        y_coords = [point[1] for point in bbox_points]
                        
                        # å°†åæ ‡ä»Žå¢žå¼ºå›¾åƒç¼©æ”¾å›žåŽŸå§‹å›¾åƒ
                        x = int(min(x_coords) / scale_factor)
                        y = int(min(y_coords) / scale_factor)
                        w = int((max(x_coords) - min(x_coords)) / scale_factor)
                        h = int((max(y_coords) - min(y_coords)) / scale_factor)
                        
                        if confidence > 0.3 and text.strip():
                            extracted_texts.append({
                                'text': text,
                                'bbox': (x, y, w, h),
                                'confidence': confidence
                            })
                            print(f"ðŸ” PaddleOCR: '{text}' (ç½®ä¿¡åº¦: {confidence:.3f})")
        
        print(f"ðŸ“Š PaddleOCRæ£€æµ‹åˆ° {len(extracted_texts)} ä¸ªæ–‡å­—åŒºåŸŸ")
        return extracted_texts
        
    except Exception as e:
        print(f"âŒ PaddleOCRè¯†åˆ«å‡ºé”™: {e}")
        print("ðŸ“‹ è°ƒè¯•ä¿¡æ¯:")
        try:
            debug_results = _paddle_ocr_instance.ocr(enhanced_img)
            print(f"  åŽŸå§‹ç»“æžœç±»åž‹: {type(debug_results)}")
            if debug_results:
                print(f"  ç»“æžœæ•°é‡: {len(debug_results)}")
                print(f"  å®Œæ•´ç»“æžœç»“æž„: {debug_results}")
                
                # å®‰å…¨åœ°æ£€æŸ¥ç¬¬ä¸€ç»„ç»“æžœ
                if len(debug_results) > 0 and debug_results[0] is not None:
                    first_group = debug_results[0]
                    print(f"  ç¬¬ä¸€ç»„ç»“æžœç±»åž‹: {type(first_group)}")
                    print(f"  ç¬¬ä¸€ç»„ç»“æžœæ•°é‡: {len(first_group) if hasattr(first_group, '__len__') else 'N/A'}")
                    
                    if hasattr(first_group, '__len__') and len(first_group) > 0:
                        first_detection = first_group[0]
                        print(f"  ç¬¬ä¸€ä¸ªæ£€æµ‹ç»“æžœ: {first_detection}")
                else:
                    print("  ç¬¬ä¸€ç»„ç»“æžœä¸ºç©ºæˆ–None")
            else:
                print("  ç»“æžœä¸ºNoneæˆ–ç©º")
        except Exception as debug_e:
            print(f"  è°ƒè¯•å¤±è´¥: {debug_e}")
            import traceback
            print(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return []

def enhance_image_for_paddle_ocr(image):
    """Optimize image for PaddleOCR.

    Returns
    -------
    Tuple[np.ndarray, float]
        Enhanced image and the scale factor applied.
    """
    height, width = image.shape[:2]
    scale_factor = max(2.0, 1000.0 / max(height, width))
    new_width = int(width * scale_factor)
    new_height = int(height * scale_factor)
    
    enhanced = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
    
    if len(enhanced.shape) == 3:
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
    
    # Enhance contrast
    enhanced = cv2.convertScaleAbs(enhanced, alpha=1.5, beta=20)
    
    # Denoise
    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)
    
    # Sharpen
    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    enhanced = cv2.filter2D(enhanced, -1, kernel)
    
    # Convert back to RGB
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2RGB)

    return enhanced, scale_factor

def extract_room_text_tesseract(image: Any) -> List[Dict]:
    """Extract room text using Tesseract (fallback)"""
    if not _HAS_TESSERACT:
        return []
    
    try:
        # Convert image to PIL format
        if isinstance(image, str):
            pil_image = Image.open(image)
        elif isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image
        
        # Enhance image
        pil_image = pil_image.convert('RGB')
        enhanced = enhance_image_for_tesseract(pil_image)
        
        # OCR
        custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzåŽ¨æˆ¿å®¢åŽ…å§å®¤å«ç”Ÿé—´é˜³å°'
        data = pytesseract.image_to_data(enhanced, lang='chi_sim+eng', config=custom_config, output_type=Output.DICT)
        
        extracted_texts = []
        n_boxes = len(data['level'])
        
        for i in range(n_boxes):
            text = data['text'][i].strip()
            conf = int(data['conf'][i])
            
            if text and conf > 30:
                x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
                extracted_texts.append({
                    'text': text,
                    'bbox': (x, y, w, h),
                    'confidence': conf / 100.0
                })
                print(f"ðŸ” Tesseract: '{text}' (ç½®ä¿¡åº¦: {conf})")
        
        print(f"ðŸ“Š Tesseractæ£€æµ‹åˆ° {len(extracted_texts)} ä¸ªæ–‡å­—åŒºåŸŸ")
        return extracted_texts
        
    except Exception as e:
        print(f"âŒ Tesseractè¯†åˆ«å‡ºé”™: {e}")
        return []

def enhance_image_for_tesseract(pil_image):
    """Optimize image for Tesseract"""
    # Resize
    width, height = pil_image.size
    scale_factor = max(2.0, 800.0 / max(width, height))
    new_size = (int(width * scale_factor), int(height * scale_factor))
    enhanced = pil_image.resize(new_size, Image.LANCZOS)
    
    # Enhance contrast
    enhancer = ImageEnhance.Contrast(enhanced)
    enhanced = enhancer.enhance(2.5)
    
    # Enhance sharpness
    enhancer = ImageEnhance.Sharpness(enhanced)
    enhanced = enhancer.enhance(2.0)
    
    return enhanced

def text_to_label(text: str) -> int:
    """Convert recognized text to room label.
    
    Returns -1 if text doesn't correspond to any known room type.
    When ENABLE_CLOSET is False, closet-related text maps to background (0).
    """
    # Clean text
    cleaned_text = ''.join(c for c in text if c.isalnum() or c in 'åŽ¨æˆ¿å®¢åŽ…å§å®¤å«ç”Ÿé—´é˜³å°')
    
    label = TEXT_LABEL_MAP.get(cleaned_text.lower(), -1)
    
    # Fuzzy matching
    if label == -1:
        for key, value in TEXT_LABEL_MAP.items():
            if key in cleaned_text or cleaned_text in key:
                label = value
                break
    
    # Handle closet disable
    if label == 1 and not ENABLE_CLOSET:
        return 0
        
    return label

def fuse_ocr_and_segmentation(seg, ocr_results):
    """Fuse OCR results with segmentation map.
    
    Parameters
    ----------
    seg: np.ndarray
        2-D segmentation labels array
    ocr_results: List[Dict]
        OCR results from extract_room_text
        
    Returns
    -------
    np.ndarray
        Fused segmentation result
    """
    fused = seg.copy()
    
    print(f"ðŸ”— èžåˆ {len(ocr_results)} ä¸ªOCRç»“æžœ")
    
    for ocr_item in ocr_results:
        text = ocr_item['text']
        confidence = ocr_item.get('confidence', 1.0)
        x, y, w, h = ocr_item['bbox']
        
        label = text_to_label(text)
        
        if label != -1:
            print(f"ðŸ“ åº”ç”¨OCRæ ‡ç­¾: '{text}' -> æ ‡ç­¾{label} (ç½®ä¿¡åº¦: {confidence:.3f})")
            
            # Ensure bbox is within image bounds
            x = max(0, min(x, seg.shape[1] - 1))
            y = max(0, min(y, seg.shape[0] - 1))
            x1 = max(0, min(x + w, seg.shape[1]))
            y1 = max(0, min(y + h, seg.shape[0]))
            
            if x1 > x and y1 > y:
                region = fused[y:y1, x:x1]
                if region.size > 0:
                    # Preserve boundaries (doors/windows and walls)
                    mask = np.isin(region, [9, 10])
                    region[~mask] = label
                    fused[y:y1, x:x1] = region
    
    # Handle closet disable globally
    if not ENABLE_CLOSET:
        fused[fused == 1] = 0
        
    return fused

# Export main functions
__all__ = ['extract_room_text', 'fuse_ocr_and_segmentation', 'text_to_label',
           'TEXT_LABEL_MAP', 'set_closet_enabled', 'ENABLE_CLOSET']
