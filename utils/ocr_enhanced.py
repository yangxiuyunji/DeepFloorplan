"""Enhanced OCR utilities with PaddleOCR support for floorplan processing."""

from typing import List, Dict, Tuple, Any
import numpy as np

# Try PaddleOCR first
try:
    from paddleocr import PaddleOCR
    import cv2
    _HAS_PADDLE_OCR = True
    _paddle_ocr_instance = None
    print("ğŸš€ PaddleOCRå¯ç”¨ï¼Œå°†ä¼˜å…ˆä½¿ç”¨")
except Exception as e:
    _HAS_PADDLE_OCR = False
    print(f"âŒ PaddleOCRå¯¼å…¥å¤±è´¥: {e}")
    print("âš ï¸ æœ¬ç¨‹åºéœ€è¦PaddleOCRæ‰èƒ½æ­£å¸¸å·¥ä½œï¼Œè¯·å®‰è£…PaddleOCR")

# Fallback to Tesseract
try:
    import pytesseract
    from pytesseract import Output
    from PIL import Image, ImageEnhance
    _HAS_TESSERACT = True
except Exception:
    pytesseract = None
    Output = None
    _HAS_TESSERACT = False

# Text to label mapping (enhanced for all room types)
TEXT_LABEL_MAP = {
    # bedroom - å§å®¤ç±» (æ ‡ç­¾4)
    'å§å®¤': 4, 'ä¸»å§': 4, 'æ¬¡å§': 4, 'å§å®¤A': 4, 'å§å®¤B': 4, 'å§å®¤C': 4, 'å§å®¤1': 4, 'å§å®¤2': 4, 'å§å®¤3': 4,
    'bedroom': 4, 'br': 4, 'bed': 4, 'master': 4, 'guest': 4,
    
    # bathroom - å«ç”Ÿé—´ç±» (æ ‡ç­¾2)  
    'å«ç”Ÿé—´': 2, 'æ´—æ‰‹é—´': 2, 'æµ´å®¤': 2, 'å«A': 2, 'å«B': 2, 'å«1': 2, 'å«2': 2,
    'bathroom': 2, 'washroom': 2, 'toilet': 2, 'wc': 2, 'bath': 2,
    
    # living/dining - å®¢å…é¤å…ç±» (æ ‡ç­¾3)
    'å®¢å…': 3, 'é¤å…': 3, 'èµ·å±…å®¤': 3, 'é¥­å…': 3, 'ä¼šå®¢å…': 3,
    'living': 3, 'livingroom': 3, 'living room': 3, 'dining': 3, 'diningroom': 3, 'dining room': 3,
    
    # balcony - é˜³å°ç±» (æ ‡ç­¾6)
    'é˜³å°': 6, 'éœ²å°': 6, 'èŠ±å›­': 6, 'åº­é™¢': 6, 'å¹³å°': 6,
    'balcony': 6, 'terrace': 6, 'patio': 6, 'garden': 6, 'deck': 6,
    
    # kitchen - å¨æˆ¿ç±» (æ ‡ç­¾7)
    'å¨æˆ¿': 7, 'å¨': 7, 'çƒ¹é¥ªé—´': 7, 'æ–™ç†å°': 7,
    'kitchen': 7, 'cook': 7, 'çƒ¹é¥ª': 7, 'kitchenette': 7,
    
    # hall/entrance - ç„å…³å¤§å…ç±» (æ ‡ç­¾5)
    'ç„å…³': 5, 'å¤§å…': 5, 'é—¨å…': 5, 'è¿‡é“': 5, 'èµ°å»Š': 5, 'å…¥å£': 5,
    'hall': 5, 'lobby': 5, 'entrance': 5, 'corridor': 5, 'foyer': 5,
    
    # closet/storage - å‚¨ç‰©é—´ç±» (æ ‡ç­¾1)
    'è¡£æŸœ': 1, 'å‚¨ç‰©é—´': 1, 'æ‚ç‰©é—´': 1, 'å‚¨è—å®¤': 1, 'è¡£å¸½é—´': 1,
    'closet': 1, 'storage': 1, 'wardrobe': 1, 'pantry': 1
}

# Global closet control - disabled by default
ENABLE_CLOSET = False

# Room type descriptions for logging
ROOM_TYPE_NAMES = {
    1: "å‚¨ç‰©é—´",
    2: "å«ç”Ÿé—´", 
    3: "å®¢å…/é¤å…",
    4: "å§å®¤",
    5: "ç„å…³/å¤§å…",
    6: "é˜³å°",
    7: "å¨æˆ¿"
}

# Room type emojis for better visualization
ROOM_TYPE_EMOJIS = {
    1: "ğŸ—„ï¸",
    2: "ğŸš¿", 
    3: "ğŸ›‹ï¸",
    4: "ğŸ›ï¸",
    5: "ğŸšª",
    6: "ğŸŒ¿",
    7: "ğŸ³"
}

def set_closet_enabled(enable: bool) -> None:
    """Globally enable or disable the closet category."""
    global ENABLE_CLOSET
    ENABLE_CLOSET = enable

def reset_paddle_ocr() -> None:
    """Reset PaddleOCR instance to apply new parameters"""
    global _paddle_ocr_instance
    _paddle_ocr_instance = None
    print("ğŸ”„ é‡ç½®PaddleOCRå®ä¾‹")

def extract_room_text(image: Any) -> List[Dict]:
    """Extract room text using PaddleOCR only.
    
    Parameters
    ----------
    image: Any
        Input image (numpy array, PIL Image, or file path)
        
    Returns
    -------
    List[Dict]
        List of detected text with bounding boxes and confidence
    """
    if not _HAS_PADDLE_OCR:
        raise RuntimeError(
            "âŒ PaddleOCRä¸å¯ç”¨ï¼è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…PaddleOCRï¼š\n"
            "   pip install paddlepaddle-gpu==2.5.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
            "   pip install paddleocr==2.6.1.3\n"
            "   æˆ–è€…ä½¿ç”¨CPUç‰ˆæœ¬ï¼špip install paddlepaddle==2.5.2"
        )
    
    print("ğŸ¯ æ­£åœ¨ä½¿ç”¨ PaddleOCR è¿›è¡Œä¸­æ–‡æ–‡å­—è¯†åˆ«...")
    return extract_room_text_paddle(image)

def extract_room_text_paddle(image: Any) -> List[Dict]:
    """Extract room text using PaddleOCR with enhanced parameters"""
    global _paddle_ocr_instance
    
    # Check if PaddleOCR is available
    if not _HAS_PADDLE_OCR:
        raise RuntimeError(
            "âŒ PaddleOCRä¸å¯ç”¨ï¼è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…PaddleOCRï¼š\n"
            "   pip install paddlepaddle-gpu==2.5.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
            "   pip install paddleocr==2.6.1.3\n"
            "   æˆ–è€…ä½¿ç”¨CPUç‰ˆæœ¬ï¼špip install paddlepaddle==2.5.2"
        )
    
    if _paddle_ocr_instance is None:
        print("ğŸš€ åˆå§‹åŒ–PaddleOCRï¼ˆå¢å¼ºæ¨¡å¼ï¼‰...")
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡æ¥å‡å°‘è­¦å‘Šä¿¡æ¯
            import os
            os.environ['PYTHONIOENCODING'] = 'utf-8'
            
            # åŸºäºPaddleOCRå®˜æ–¹æ–‡æ¡£çš„æœ€ä½³å‚æ•°é…ç½®
            _paddle_ocr_instance = PaddleOCR(
                lang='ch',
                det_db_thresh=0.2,        # åƒç´ åˆ†ç±»é˜ˆå€¼ï¼Œè¶Šå°æ£€æµ‹è¶Šå¤šæ–‡æœ¬ï¼ˆå®˜æ–¹é»˜è®¤0.3ï¼‰
                det_db_box_thresh=0.4,    # æ–‡æœ¬æ¡†ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå‡å°‘æ¼æ£€ï¼ˆå®˜æ–¹é»˜è®¤0.6ï¼‰
                det_db_unclip_ratio=2.5,  # æ–‡æœ¬æ¡†æ‰©å¼ ç³»æ•°ï¼Œå®˜æ–¹æ¨è2.5é¿å…è¾¹ç¼˜ä¸¢å¤±
                drop_score=0.3,           # è¯†åˆ«ç»“æœç½®ä¿¡åº¦è¿‡æ»¤ï¼ˆé»˜è®¤0.5ï¼‰
                use_angle_cls=True,       # å¯ç”¨è§’åº¦åˆ†ç±»å™¨å¤„ç†æ—‹è½¬æ–‡å­—
                cls_thresh=0.8,           # è§’åº¦åˆ†ç±»é˜ˆå€¼ï¼ˆé»˜è®¤0.9ï¼‰
                use_dilation=True,        # å®˜æ–¹æ¨è: è†¨èƒ€å¤„ç†æé«˜æ£€æµ‹æ•ˆæœ
                det_db_score_mode='slow'  # å®˜æ–¹æ¨è: æ›´ç²¾ç¡®çš„å¾—åˆ†è®¡ç®—æ¨¡å¼
            )
            print("âœ… PaddleOCRä¸“ä¸šä¼˜åŒ–æ¨¡å¼åˆå§‹åŒ–å®Œæˆ")
            print("   ğŸ“‹ å®˜æ–¹æœ€ä½³å®è·µå‚æ•°:")
            print("   ğŸ”¹ det_db_thresh=0.2 (æ›´æ•æ„Ÿçš„åƒç´ æ£€æµ‹)")
            print("   ğŸ”¹ det_db_box_thresh=0.4 (å‡å°‘æ¼æ£€)")
            print("   ğŸ”¹ det_db_unclip_ratio=2.5 (å®˜æ–¹æ¨èæ‰©å¼ ç³»æ•°)")
            print("   ğŸ”¹ use_dilation=True (è†¨èƒ€å¤„ç†æå‡æ•ˆæœ)")
            print("   ğŸ”¹ det_db_score_mode='slow' (ç²¾ç¡®å¾—åˆ†è®¡ç®—)")
        except Exception as e:
            print(f"âŒ PaddleOCRåˆå§‹åŒ–å¤±è´¥: {e}")
            return []
    
    # Process image
    if isinstance(image, str):
        img = cv2.imread(image)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    elif isinstance(image, np.ndarray):
        img = image.copy()
    else:
        img = np.array(image)
    
    # Enhance image for better OCR and keep scale factor for coordinate correction
    enhanced_img, scale_factor = enhance_image_for_paddle_ocr(img)
    
    try:
        results = _paddle_ocr_instance.ocr(enhanced_img)
        extracted_texts = []
        
        # å¤„ç†æ–°çš„PaddleOCRç»“æœæ ¼å¼
        if results and len(results) > 0:
            result = results[0]  # è·å–ç¬¬ä¸€ä¸ªç»“æœ
            
            # å°è¯•ç›´æ¥è®¿é—®ç»“æœæ•°æ®
            if 'rec_texts' in result:
                texts = result['rec_texts']
                scores = result['rec_scores']  
                polys = result['rec_polys']
                
                print(f"ğŸ” PaddleOCRæ£€æµ‹åˆ°æ–‡æœ¬: {texts}")
                
                for i, (text, score, poly) in enumerate(zip(texts, scores, polys)):
                    # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„å•å­—ç¬¦è¯†åˆ«
                    if len(text.strip()) == 1 and score < 0.7:
                        print(f"ğŸš« [OCRè¿‡æ»¤] è¿‡æ»¤ä½ç½®ä¿¡åº¦å•å­—ç¬¦: '{text}' (ç½®ä¿¡åº¦: {score:.3f})")
                        continue
                        
                    if score > 0.3 and text.strip():
                        # ä»è¾¹ç•Œæ¡†å¤šè¾¹å½¢è®¡ç®—çŸ©å½¢è¾¹ç•Œ
                        x_coords = [point[0] for point in poly]
                        y_coords = [point[1] for point in poly]
                        
                        # åæ ‡åŸºäºå¢å¼ºåçš„å›¾åƒï¼Œéœ€è¦ç¼©æ”¾å›åŸå§‹å°ºå¯¸
                        x = int(min(x_coords) / scale_factor)
                        y = int(min(y_coords) / scale_factor)
                        w = int((max(x_coords) - min(x_coords)) / scale_factor)
                        h = int((max(y_coords) - min(y_coords)) / scale_factor)
                        
                        extracted_texts.append({
                            'text': text,
                            'bbox': (x, y, w, h),
                            'confidence': score
                        })
                        print(f"ğŸ” PaddleOCR: '{text}' (ç½®ä¿¡åº¦: {score:.3f})")
            
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            elif isinstance(result, list):
                for line in result:
                    if len(line) >= 2:
                        bbox_points = line[0]
                        text_info = line[1]
                        
                        if isinstance(text_info, (list, tuple)) and len(text_info) >= 2:
                            text = text_info[0]
                            confidence = text_info[1]
                        else:
                            text = str(text_info)
                            confidence = 1.0
                        
                        # Convert bbox format
                        x_coords = [point[0] for point in bbox_points]
                        y_coords = [point[1] for point in bbox_points]
                        
                        # å°†åæ ‡ä»å¢å¼ºå›¾åƒç¼©æ”¾å›åŸå§‹å›¾åƒ
                        x = int(min(x_coords) / scale_factor)
                        y = int(min(y_coords) / scale_factor)
                        w = int((max(x_coords) - min(x_coords)) / scale_factor)
                        h = int((max(y_coords) - min(y_coords)) / scale_factor)
                        
                        if confidence > 0.3 and text.strip():
                            extracted_texts.append({
                                'text': text,
                                'bbox': (x, y, w, h),
                                'confidence': confidence
                            })
                            print(f"ğŸ” PaddleOCR: '{text}' (ç½®ä¿¡åº¦: {confidence:.3f})")
        
        print(f"ğŸ“Š PaddleOCRæ£€æµ‹åˆ° {len(extracted_texts)} ä¸ªæ–‡å­—åŒºåŸŸ")
        return extracted_texts
        
    except Exception as e:
        print(f"âŒ PaddleOCRè¯†åˆ«å‡ºé”™: {e}")
        print("ğŸ“‹ è°ƒè¯•ä¿¡æ¯:")
        try:
            debug_results = _paddle_ocr_instance.ocr(enhanced_img)
            print(f"  åŸå§‹ç»“æœç±»å‹: {type(debug_results)}")
            if debug_results:
                print(f"  ç»“æœæ•°é‡: {len(debug_results)}")
                print(f"  å®Œæ•´ç»“æœç»“æ„: {debug_results}")
                
                # å®‰å…¨åœ°æ£€æŸ¥ç¬¬ä¸€ç»„ç»“æœ
                if len(debug_results) > 0 and debug_results[0] is not None:
                    first_group = debug_results[0]
                    print(f"  ç¬¬ä¸€ç»„ç»“æœç±»å‹: {type(first_group)}")
                    print(f"  ç¬¬ä¸€ç»„ç»“æœæ•°é‡: {len(first_group) if hasattr(first_group, '__len__') else 'N/A'}")
                    
                    if hasattr(first_group, '__len__') and len(first_group) > 0:
                        first_detection = first_group[0]
                        print(f"  ç¬¬ä¸€ä¸ªæ£€æµ‹ç»“æœ: {first_detection}")
                else:
                    print("  ç¬¬ä¸€ç»„ç»“æœä¸ºç©ºæˆ–None")
            else:
                print("  ç»“æœä¸ºNoneæˆ–ç©º")
        except Exception as debug_e:
            print(f"  è°ƒè¯•å¤±è´¥: {debug_e}")
            import traceback
            print(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return []

def enhance_image_for_paddle_ocr(image):
    """Optimize image for PaddleOCR.

    Returns
    -------
    Tuple[np.ndarray, float]
        Enhanced image and the scale factor applied.
    """
    height, width = image.shape[:2]
    scale_factor = max(2.0, 1000.0 / max(height, width))
    new_width = int(width * scale_factor)
    new_height = int(height * scale_factor)
    
    enhanced = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
    
    if len(enhanced.shape) == 3:
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
    
    # Enhance contrast
    enhanced = cv2.convertScaleAbs(enhanced, alpha=1.5, beta=20)
    
    # Denoise
    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)
    
    # Sharpen
    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    enhanced = cv2.filter2D(enhanced, -1, kernel)
    
    # Convert back to RGB
    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2RGB)

    return enhanced, scale_factor

def extract_room_text_tesseract(image: Any) -> List[Dict]:
    """Extract room text using Tesseract (fallback)"""
    if not _HAS_TESSERACT:
        return []
    
    try:
        # Convert image to PIL format
        if isinstance(image, str):
            pil_image = Image.open(image)
        elif isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = image
        
        # Enhance image
        pil_image = pil_image.convert('RGB')
        enhanced = enhance_image_for_tesseract(pil_image)
        
        # OCR
        custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzå¨æˆ¿å®¢å…å§å®¤å«ç”Ÿé—´é˜³å°'
        data = pytesseract.image_to_data(enhanced, lang='chi_sim+eng', config=custom_config, output_type=Output.DICT)
        
        extracted_texts = []
        n_boxes = len(data['level'])
        
        for i in range(n_boxes):
            text = data['text'][i].strip()
            conf = int(data['conf'][i])
            
            if text and conf > 30:
                x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
                extracted_texts.append({
                    'text': text,
                    'bbox': (x, y, w, h),
                    'confidence': conf / 100.0
                })
                print(f"ğŸ” Tesseract: '{text}' (ç½®ä¿¡åº¦: {conf})")
        
        print(f"ğŸ“Š Tesseractæ£€æµ‹åˆ° {len(extracted_texts)} ä¸ªæ–‡å­—åŒºåŸŸ")
        return extracted_texts
        
    except Exception as e:
        print(f"âŒ Tesseractè¯†åˆ«å‡ºé”™: {e}")
        return []

def enhance_image_for_tesseract(pil_image):
    """Optimize image for Tesseract"""
    # Resize
    width, height = pil_image.size
    scale_factor = max(2.0, 800.0 / max(width, height))
    new_size = (int(width * scale_factor), int(height * scale_factor))
    enhanced = pil_image.resize(new_size, Image.LANCZOS)
    
    # Enhance contrast
    enhancer = ImageEnhance.Contrast(enhanced)
    enhanced = enhancer.enhance(2.5)
    
    # Enhance sharpness
    enhancer = ImageEnhance.Sharpness(enhanced)
    enhanced = enhancer.enhance(2.0)
    
    return enhanced

def text_to_label(text: str) -> int:
    """Convert recognized text to room label.
    
    Returns -1 if text doesn't correspond to any known room type.
    When ENABLE_CLOSET is False, closet-related text maps to background (0).
    """
    # Clean text
    cleaned_text = ''.join(c for c in text if c.isalnum() or c in 'å¨æˆ¿å®¢å…å§å®¤å«ç”Ÿé—´é˜³å°')
    
    label = TEXT_LABEL_MAP.get(cleaned_text.lower(), -1)
    
    # Fuzzy matching
    if label == -1:
        for key, value in TEXT_LABEL_MAP.items():
            if key in cleaned_text or cleaned_text in key:
                label = value
                break
    
    # Handle closet disable
    if label == 1 and not ENABLE_CLOSET:
        return 0
        
    return label

def fuse_ocr_and_segmentation(seg, ocr_results):
    """Fuse OCR results with segmentation map using region growing."""

    from collections import deque

    fused = seg.copy()

    print(f"ğŸ”— èåˆ {len(ocr_results)} ä¸ªOCRç»“æœ")

    barriers = {9, 10}
    door_threshold = 10  # maximum width treated as door

    def flood_fill(seed_x: int, seed_y: int) -> np.ndarray:
        """Perform BFS flood fill with simple door detection."""
        h, w = seg.shape
        visited = np.zeros((h, w), dtype=bool)
        mask = np.zeros((h, w), dtype=bool)
        q = deque([(seed_x, seed_y)])

        while q:
            x, y = q.popleft()
            if x < 0 or x >= w or y < 0 or y >= h:
                continue
            if visited[y, x]:
                continue
            visited[y, x] = True
            if seg[y, x] in barriers:
                continue
            mask[y, x] = True

            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                nx, ny = x + dx, y + dy
                if nx < 0 or nx >= w or ny < 0 or ny >= h:
                    continue
                if visited[ny, nx] or seg[ny, nx] in barriers:
                    continue

                # Door detection: measure corridor width perpendicular to movement
                if dx != 0:
                    span = 1
                    while True:
                        up, down = ny - span, ny + span
                        if up < 0 or down >= h:
                            break
                        if seg[up, nx] in barriers or seg[down, nx] in barriers:
                            break
                        span += 1
                    corridor = span * 2 - 1
                else:
                    span = 1
                    while True:
                        left, right = nx - span, nx + span
                        if left < 0 or right >= w:
                            break
                        if seg[ny, left] in barriers or seg[ny, right] in barriers:
                            break
                        span += 1
                    corridor = span * 2 - 1

                if corridor <= door_threshold:
                    mask[ny, nx] = True
                    visited[ny, nx] = True
                    continue

                q.append((nx, ny))

        return mask

    for ocr_item in ocr_results:
        text = ocr_item['text']
        confidence = ocr_item.get('confidence', 1.0)
        x, y, w, h = ocr_item['bbox']

        label = text_to_label(text)

        if label == -1:
            continue

        print(f"ğŸ“ åº”ç”¨OCRæ ‡ç­¾: '{text}' -> æ ‡ç­¾{label} (ç½®ä¿¡åº¦: {confidence:.3f})")

        # Seed at text center
        cx = x + w // 2
        cy = y + h // 2
        cx = max(0, min(cx, seg.shape[1] - 1))
        cy = max(0, min(cy, seg.shape[0] - 1))

        if seg[cy, cx] in barriers:
            # Try to find non-barrier pixel within bbox
            found = False
            for yy in range(max(0, y), min(seg.shape[0], y + h)):
                for xx in range(max(0, x), min(seg.shape[1], x + w)):
                    if seg[yy, xx] not in barriers:
                        cx, cy = xx, yy
                        found = True
                        break
                if found:
                    break
            if not found:
                continue

        room_mask = flood_fill(cx, cy)
        fused[room_mask] = label

    if not ENABLE_CLOSET:
        fused[fused == 1] = 0

    return fused

# Export main functions
__all__ = ['extract_room_text', 'fuse_ocr_and_segmentation', 'text_to_label',
           'TEXT_LABEL_MAP', 'set_closet_enabled', 'ENABLE_CLOSET']
